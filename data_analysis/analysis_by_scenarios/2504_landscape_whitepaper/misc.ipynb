{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01f1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "json_file_path = 'hf_top_models.json'  # Path to your JSON file\n",
    "csv_file_path = 'hf_top_models.csv'  # Path to save the CSV file\n",
    "try:\n",
    "  with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "  # Convert to DataFrame\n",
    "  if isinstance(data, list):\n",
    "    df = pd.DataFrame(data)\n",
    "  else:\n",
    "    # If it's a single object or has a nested structure\n",
    "    df = pd.json_normalize(data)\n",
    "\n",
    "  # Write to CSV\n",
    "  df.to_csv(csv_file_path, index=False, encoding='utf-8-sig')\n",
    "  print(f\"Successfully converted {json_file_path} to {csv_file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c947852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiaoyawork/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import clickhouse_connect\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "clickhouse_host = os.getenv(\"CLICKHOUSE_HOST\")\n",
    "username = os.getenv(\"CLICKHOUSE_USER\")\n",
    "password = os.getenv(\"CLICKHOUSE_PASSWORD\")\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "client = clickhouse_connect.get_client(host=clickhouse_host, port=8123, username=username, password=password)\n",
    "\n",
    "headers = {\n",
    "  \"Authorization\": f\"token {github_token}\"\n",
    "}\n",
    "\n",
    "# 查询 github openrank top 项目列表\n",
    "def execute_query_top_openrank(created_at='2025-01-01'):\n",
    "  sql_query_top_openrank = \"\"\"\n",
    "    SELECT\n",
    "        repo_id,\n",
    "        repo_name,\n",
    "        ROUND(AVG(openrank), 2) AS avg_openrank_25\n",
    "    FROM\n",
    "        opensource.global_openrank\n",
    "    WHERE\n",
    "        platform = 'GitHub' AND\n",
    "        created_at >= %s\n",
    "    GROUP BY\n",
    "        repo_id, repo_name\n",
    "    ORDER BY\n",
    "        avg_openrank_25 DESC\n",
    "    LIMIT 500\n",
    "  \"\"\"\n",
    "  formatted_query = sql_query_top_openrank % (f\"'{created_at}'\")\n",
    "  results = client.query(formatted_query)\n",
    "  return results\n",
    "\n",
    "# 根据 repo_name, 通过 GitHub API 查询仓库基本信息\n",
    "def get_repo_info(repo_name, headers):\n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        repo_info = {\n",
    "            \"repo_id\": data.get(\"id\"),\n",
    "            \"repo_name\": data.get(\"full_name\"),\n",
    "            \"stargazers_count\": data.get(\"stargazers_count\"),\n",
    "            \"forks_count\": data.get(\"forks_count\"),\n",
    "            \"language\": data.get(\"language\"),\n",
    "            \"created_at\": data.get(\"created_at\").split(\"T\")[0],\n",
    "            \"description\": data.get(\"description\"),\n",
    "            \"topics\": data.get(\"topics\", []),\n",
    "        }\n",
    "        return repo_info\n",
    "    else:\n",
    "        print(f\"Error fetching data for {repo_name}: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NixOS/nixpkgs...\n",
      "Processing llvm/llvm-project...\n",
      "Processing home-assistant/core...\n",
      "Processing CypherV2/Donarev-API...\n",
      "Error fetching data for CypherV2/Donarev-API: 404\n",
      "Failed to get info for CypherV2/Donarev-API\n",
      "Processing pytorch/pytorch...\n",
      "Processing odoo/odoo...\n",
      "Processing 7Sence/Rise-Beta...\n",
      "Error fetching data for 7Sence/Rise-Beta: 404\n",
      "Failed to get info for 7Sence/Rise-Beta\n",
      "Processing zephyrproject-rtos/zephyr...\n",
      "Processing DXVVAY/hcaptcha-reverse...\n",
      "Error fetching data for DXVVAY/hcaptcha-reverse: 404\n",
      "Failed to get info for DXVVAY/hcaptcha-reverse\n",
      "Processing microsoft/vscode...\n",
      "Processing microsoft/winget-pkgs...\n",
      "Processing elastic/kibana...\n",
      "Processing DigitalPlatDev/FreeDomain...\n",
      "Processing vllm-project/vllm...\n",
      "Processing godotengine/godot...\n",
      "Processing justachillcoder/binance-captcha-deobfuscator...\n",
      "Error fetching data for justachillcoder/binance-captcha-deobfuscator: 404\n",
      "Failed to get info for justachillcoder/binance-captcha-deobfuscator\n",
      "Processing flutter/flutter...\n",
      "Processing Expensify/App...\n",
      "Processing CSolverV2/hCaptcha-ID-Decoder...\n",
      "Error fetching data for CSolverV2/hCaptcha-ID-Decoder: 404\n",
      "Failed to get info for CSolverV2/hCaptcha-ID-Decoder\n",
      "Processing emrovsky/x-ms-reference-id...\n",
      "Error fetching data for emrovsky/x-ms-reference-id: 404\n",
      "Failed to get info for emrovsky/x-ms-reference-id\n",
      "Processing emrovsky/hcaptcha-blob-encryption...\n",
      "Error fetching data for emrovsky/hcaptcha-blob-encryption: 404\n",
      "Failed to get info for emrovsky/hcaptcha-blob-encryption\n",
      "Processing zed-industries/zed...\n",
      "Processing department-of-veterans-affairs/va.gov-team...\n",
      "Processing grafana/grafana...\n",
      "Processing rust-lang/rust...\n",
      "Processing ghscr/ghscription...\n",
      "Error fetching data for ghscr/ghscription: 404\n",
      "Failed to get info for ghscr/ghscription\n",
      "Processing langgenius/dify...\n",
      "Processing digitalinnovationone/dio-lab-open-source...\n",
      "Processing openjdk/jdk...\n",
      "Processing WebKit/WebKit...\n",
      "Processing is-a-dev/register...\n",
      "Processing Kas-tle/java2bedrock.sh...\n",
      "Processing Automattic/wp-calypso...\n",
      "Processing ClickHouse/ClickHouse...\n",
      "Processing dotnet/runtime...\n",
      "Processing dropout1337/wasm-commenter...\n",
      "Error fetching data for dropout1337/wasm-commenter: 404\n",
      "Failed to get info for dropout1337/wasm-commenter\n",
      "Processing openshift/openshift-docs...\n",
      "Processing python/cpython...\n",
      "Processing openshift/release...\n",
      "Processing getsentry/sentry...\n",
      "Processing leanprover-community/mathlib4...\n",
      "Processing elastic/elasticsearch...\n",
      "Processing tenstorrent/tt-metal...\n",
      "Processing firstcontributions/first-contributions...\n",
      "Processing oven-sh/bun...\n",
      "Processing zen-browser/desktop...\n",
      "Processing DataDog/datadog-agent...\n",
      "Processing expo/expo...\n",
      "Processing huggingface/transformers...\n",
      "Processing vercel/next.js...\n",
      "Processing JetBrains/swot...\n",
      "Processing microsoft/vscode-copilot-release...\n",
      "Processing kubernetes/kubernetes...\n",
      "Processing ollama/ollama...\n",
      "Processing CherryHQ/cherry-studio...\n",
      "Processing woocommerce/woocommerce...\n",
      "Processing sgl-project/sglang...\n",
      "Processing raycast/extensions...\n",
      "Processing apache/doris...\n",
      "Processing ceph/ceph...\n",
      "Processing apache/airflow...\n",
      "Processing openvinotoolkit/openvino...\n",
      "Processing astral-sh/uv...\n",
      "Processing AG-597/Csolver...\n",
      "Error fetching data for AG-597/Csolver: 404\n",
      "Failed to get info for AG-597/Csolver\n",
      "Processing space-wizards/space-station-14...\n",
      "Processing DataDog/documentation...\n",
      "Processing ydb-platform/ydb...\n",
      "Processing BerriAI/litellm...\n",
      "Processing AliceO2Group/O2Physics...\n",
      "Processing open-webui/open-webui...\n",
      "Processing Azure/azure-rest-api-specs...\n",
      "Processing metabase/metabase...\n",
      "Processing ggml-org/llama.cpp...\n",
      "Processing department-of-veterans-affairs/vets-website...\n",
      "Processing FreeCAD/FreeCAD...\n",
      "Processing cloudflare/cloudflare-docs...\n",
      "Processing tgstation/tgstation...\n",
      "Processing openwrt/openwrt...\n",
      "Processing ultralytics/ultralytics...\n",
      "Processing ggerganov/llama.cpp...\n",
      "Processing cilium/cilium...\n",
      "Processing ray-project/ray...\n",
      "Processing dotnet/maui...\n",
      "Processing demisto/content...\n",
      "Processing nrfconnect/sdk-nrf...\n",
      "Processing n8n-io/n8n...\n",
      "Processing Koenkk/zigbee2mqtt...\n",
      "Processing openjournals/joss-reviews...\n",
      "Processing ArweaveOasis/Arweave-Academy...\n",
      "Processing department-of-veterans-affairs/vets-api...\n",
      "Processing intel/llvm...\n",
      "Processing microsoft/PowerToys...\n",
      "Processing PaddlePaddle/Paddle...\n",
      "Processing bitwarden/clients...\n",
      "Processing swiftlang/swift...\n",
      "Processing Automattic/jetpack...\n",
      "Processing cockroachdb/cockroach...\n",
      "Processing StarRocks/starrocks...\n",
      "Processing infiniflow/ragflow...\n",
      "Successfully processed 89 repositories\n",
      "Results saved to repo_info_with_openrank_20250630.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "repo_names = [\n",
    "  \n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  print(f\"Processing {repo_name}...\")\n",
    "  \n",
    "  # Get repo info from GitHub API\n",
    "  repo_info = get_repo_info(repo_name, headers)\n",
    "  \n",
    "  if repo_info:\n",
    "        \n",
    "    # Compile all information\n",
    "    result = {\n",
    "      \"repo_name\": repo_info[\"repo_name\"],\n",
    "      \"stars\": repo_info[\"stargazers_count\"],\n",
    "      \"forks\": repo_info[\"forks_count\"],\n",
    "      \"language\": repo_info[\"language\"],\n",
    "      \"created_at\": repo_info[\"created_at\"].split(\"T\")[0],\n",
    "      \"description\": repo_info[\"description\"],\n",
    "      \"topics\": \", \".join(repo_info.get(\"topics\", [])) if repo_info.get(\"topics\") else \"\"\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "  else:\n",
    "    print(f\"Failed to get info for {repo_name}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "columns_order = [\"repo_name\", \"stars\", \"forks\", \"language\", \"created_at\", \"description\", 'topics']\n",
    "df_results = df_results[columns_order]\n",
    "\n",
    "csv_filename = f\"repo_info_with_openrank_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Successfully processed {len(results)} repositories\")\n",
    "print(f\"Results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2517e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a5eee1",
   "metadata": {},
   "source": [
    "### 通过 oss 读取指标数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to import requests again since it's already imported in cell 1\n",
    "import requests\n",
    "import pandas as pd  \n",
    "\n",
    "def fetch_repo_info(repo_names):\n",
    "  repo_data = []\n",
    "  \n",
    "  for repo_name in repo_names:\n",
    "    # Format the URLs with the repo name for each iteration\n",
    "    contributors_url = f\"https://oss.open-digger.cn/github/{repo_name}/contributors.json\"\n",
    "    participants_url = f\"https://oss.open-digger.cn/github/{repo_name}/participants.json\"\n",
    "    \n",
    "    try:\n",
    "      contributors_response = requests.get(contributors_url, timeout=10)\n",
    "      participants_response = requests.get(participants_url, timeout=10)\n",
    "      \n",
    "      if contributors_response.status_code == 200 and participants_response.status_code == 200:\n",
    "        contributors_json = contributors_response.json()\n",
    "        participants_json = participants_response.json()\n",
    "        contributors_2025 = contributors_json.get('2025')\n",
    "        participants_2025 = participants_json.get('2025')\n",
    "        \n",
    "        repo_info = {\n",
    "          'repo_name': repo_name,\n",
    "          'contributors_2025': contributors_2025 if contributors_2025 else 0,\n",
    "          'participants_2025': participants_2025 if participants_2025 else 0\n",
    "        }\n",
    "        repo_data.append(repo_info)\n",
    "        print(f\"Processed {repo_name}\")\n",
    "      else:\n",
    "        print(f\"Error fetching data for {repo_name}\")\n",
    "    except Exception as e:\n",
    "      print(f\"Exception for {repo_name}: {e}\")\n",
    "    \n",
    "    # Small delay to avoid overwhelming the API\n",
    "    time.sleep(0.5)\n",
    "  \n",
    "  return repo_data\n",
    "\n",
    "repo_names = [\n",
    "  \"pytorch/pytorch\",\n",
    "  \"vllm-project/vllm\",\n",
    "  \"langgenius/dify\",\n",
    "  \"elastic/elasticsearch\",\n",
    "  \"ollama/ollama\",\n",
    "  \"openvinotoolkit/openvino\",\n",
    "  \"apache/airflow\",\n",
    "  \"sgl-project/sglang\",\n",
    "  \"open-webui/open-webui\",\n",
    "  \"ggml-org/llama.cpp\",\n",
    "  \"BerriAI/litellm\",\n",
    "  \"ray-project/ray\",\n",
    "  \"stackblitz/bolt.new\",\n",
    "  \"PaddlePaddle/Paddle\",\n",
    "  \"airbytehq/airbyte\",\n",
    "  \"apache/spark\",\n",
    "  \"microsoft/onnxruntime\",\n",
    "  \"n8n-io/n8n\",\n",
    "  \"infiniflow/ragflow\",\n",
    "  \"NVIDIA/NeMo\"\n",
    "]\n",
    "# Call the function with repo_names\n",
    "collected_data = fetch_repo_info(repo_names)\n",
    "\n",
    "# Convert to dataframe and save to CSV\n",
    "result_df = pd.DataFrame(collected_data)\n",
    "result_df.to_csv('repo_metrics_2025.csv', index=False)\n",
    "print(f\"Saved data for {len(result_df)} repositories to repo_metrics_2025.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

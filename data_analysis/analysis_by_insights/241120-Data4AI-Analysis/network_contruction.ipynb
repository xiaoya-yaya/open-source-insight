{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# 连接 Clickhouse 数据库\n",
    "import clickhouse_connect\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "clickhouse_host = os.getenv(\"CLICKHOUSE_HOST\")\n",
    "username = os.getenv(\"CLICKHOUSE_USER\")\n",
    "password = os.getenv(\"CLICKHOUSE_PASSWORD\")\n",
    "github_token = os.getenv(\"GitHub_API_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = clickhouse_connect.get_client(host=clickhouse_host, port=8123, username=username, password=password)\n",
    "\n",
    "def execute_search_projects(project_ids, limit):   \n",
    "    sql_search_projects = \"\"\" \n",
    "    WITH\n",
    "        -- 获取仓库在指定时间段内的活跃开发者\n",
    "        active_developers AS (\n",
    "            SELECT DISTINCT actor_id, repo_id, repo_name\n",
    "            FROM opensource.events\n",
    "            WHERE repo_id IN (%s)\n",
    "            AND created_at >= '2024-01-01'\n",
    "            AND created_at < '2024-11-01'\n",
    "            AND (type IN ('IssuesEvent', 'PullRequestEvent', 'IssueCommentEvent', 'PullRequestReviewEvent','PullRequestReviewCommentEvent'))\n",
    "        ),\n",
    "        -- 计算其他仓库中的活跃开发者\n",
    "        repo_activity AS (\n",
    "            SELECT repo_id, repo_name, COUNT(DISTINCT actor_id) AS active_count\n",
    "            FROM opensource.events\n",
    "            WHERE actor_id IN (SELECT actor_id FROM active_developers)\n",
    "            AND created_at >= '2024-01-01'\n",
    "            AND created_at < '2024-11-01'\n",
    "            AND repo_id NOT IN (%s)\n",
    "            AND (type IN ('IssuesEvent', 'PullRequestEvent', 'IssueCommentEvent','PullRequestReviewEvent', 'PullRequestReviewCommentEvent'))\n",
    "            GROUP BY repo_id, repo_name\n",
    "        )\n",
    "        -- 获取关联度最高的 x 个仓库\n",
    "        SELECT repo_id, repo_name, active_count\n",
    "        FROM repo_activity\n",
    "        ORDER BY active_count DESC\n",
    "        LIMIT %s\n",
    "        \"\"\"\n",
    "    formatted_query = sql_search_projects % (', '.join(f\"'{id}'\" for id in project_ids), ', '.join(f\"'{id}'\" for id in project_ids), limit)\n",
    "    results = client.query(formatted_query)\n",
    "    return results\n",
    "        \n",
    "\n",
    "def execute_query_relations(repo_id1, repo_id2):\n",
    "    sql_query_relations = \"\"\" \n",
    "        -- 计算两个仓库之间的共同开发者数量\n",
    "        WITH\n",
    "            -- 获取第一个仓库在指定时间段内的活跃开发者\n",
    "            active_developers_1 AS (\n",
    "                SELECT DISTINCT actor_id\n",
    "                FROM opensource.events\n",
    "                WHERE repo_id = %s\n",
    "                AND created_at >= '2024-01-01'\n",
    "                AND created_at < '2024-11-01'\n",
    "                AND (type IN ('IssuesEvent', 'PullRequestEvent', 'IssueCommentEvent','PullRequestReviewEvent','PullRequestReviewCommentEvent'))\n",
    "            ),\n",
    "            -- 获取第二个仓库在指定时间段内的活跃开发者\n",
    "            active_developers_2 AS (\n",
    "                SELECT DISTINCT actor_id\n",
    "                FROM opensource.events\n",
    "                WHERE repo_id = %s\n",
    "                AND created_at >= '2024-01-01'\n",
    "                AND created_at < '2024-11-01'\n",
    "                AND (type IN ('IssuesEvent', 'PullRequestEvent', 'IssueCommentEvent', 'PullRequestReviewEvent', 'PullRequestReviewCommentEvent'))\n",
    "            )\n",
    "        -- 计算共同开发者数量\n",
    "        SELECT COUNT(DISTINCT a.actor_id) AS common_developer_count\n",
    "        FROM active_developers_1 a\n",
    "        JOIN active_developers_2 b ON a.actor_id = b.actor_id\n",
    "        \"\"\"\n",
    "    formatted_query = sql_query_relations % (f\"'{repo_id1}'\", f\"'{repo_id2}'\")\n",
    "    results = client.query(formatted_query)\n",
    "    return results\n",
    "\n",
    "def execute_nodes_openrank(repo_id):\n",
    "    sql_nodes_openrank = \"\"\"\n",
    "        -- 根据 repo_id 查询仓库的 openrank 均值\n",
    "        SELECT repo_id, avg(openrank) AS average_openrank\n",
    "        FROM opensource.global_openrank\n",
    "        WHERE repo_id = %s\n",
    "        AND platform = 'GitHub'\n",
    "        AND created_at >= '2024-01-01'\n",
    "        AND created_at < '2024-11-01'\n",
    "        GROUP BY repo_id\n",
    "    \"\"\"\n",
    "    formatted_query = sql_nodes_openrank % (f\"'{repo_id}'\")\n",
    "    results = client.query(formatted_query)\n",
    "    return results\n",
    "\n",
    "def execute_query_projects_name(repo_id):\n",
    "    sql_query_project_name = \"\"\"\n",
    "        -- 根据 repo_id 查询最新的 repo_name\n",
    "        SELECT repo_id, repo_name\n",
    "        FROM opensource.events\n",
    "        WHERE repo_id IN (%s)\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "    formatted_query = sql_query_project_name % (f\"'{repo_id}'\")\n",
    "    results = client.query(formatted_query)\n",
    "    return results\n",
    "\n",
    "def execute_query_projects_id(repo_name):\n",
    "    sql_query_project_name = \"\"\"\n",
    "        -- 根据 repo_name 查询最新的 repo_id\n",
    "        SELECT repo_id, repo_name\n",
    "        FROM opensource.events\n",
    "        WHERE repo_name IN (%s)\n",
    "        ORDER BY created_at DESC\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "    formatted_query = sql_query_project_name % (f\"'{repo_name}'\")\n",
    "    results = client.query(formatted_query)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('big_data_projects_full.csv')\n",
    "repo_names = df['repository'].tolist()\n",
    "repo_names = set(repo_names)\n",
    "repo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "  'Authorization': f'token {github_token}',\n",
    "  'Accept': 'application/vnd.github.v3+json',\n",
    "}\n",
    "\n",
    "final_projects = {}\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  url = f'https://api.github.com/repos/{repo_name}'\n",
    "  response = requests.get(url, headers=headers)\n",
    "  if response.status_code == 200:\n",
    "    repo_data = response.json()\n",
    "    repo_id = repo_data['id']\n",
    "    final_projects[repo_id] = {\"name\": repo_name, \"openrank\": None}\n",
    "    print(final_projects[repo_id])\n",
    "  else:\n",
    "    print(f\"Failed to fetch data for {repo_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo_id in list(final_projects.keys()):\n",
    "    openrank = execute_nodes_openrank(repo_id)\n",
    "    if openrank and openrank.result_rows:\n",
    "        print(openrank.result_rows)\n",
    "        average_openrank = int(openrank.result_rows[0][1])\n",
    "        final_projects[repo_id][\"openrank\"] = average_openrank\n",
    "        if average_openrank >= 10:\n",
    "            final_projects[repo_id][\"openrank\"] = average_openrank\n",
    "        else:\n",
    "            del final_projects[repo_id]\n",
    "    else:\n",
    "        del final_projects[repo_id]\n",
    "\n",
    "nodes = []\n",
    "for repo_id, info in final_projects.items():\n",
    "    nodes.append([info[\"name\"], info[\"openrank\"]])\n",
    "\n",
    "edges = []\n",
    "for key1, key2 in itertools.combinations(final_projects.keys(), 2):\n",
    "    project1 = final_projects[key1]['name']\n",
    "    project2 = final_projects[key2]['name']\n",
    "\n",
    "    query_result = execute_query_relations(key1, key2)\n",
    "    if query_result.result_rows:\n",
    "        value = query_result.result_rows[0][0]\n",
    "        edges.append([project1, project2, value])\n",
    "\n",
    "filtered_edges = [edge for edge in edges if edge[2] >= 10]\n",
    "\n",
    "print(final_projects)\n",
    "print(len(nodes),len(filtered_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 389\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes), len(filtered_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 nodes 和 edges 组合并存储到 JSON 文件\n",
    "graph = {\n",
    "    \"nodes\": nodes,\n",
    "    \"edges\": filtered_edges\n",
    "}\n",
    "\n",
    "# 保存到 graph.json 文件中\n",
    "with open('graph_data_engineering_2024.json', 'w') as json_file:\n",
    "    json.dump(graph, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 openrank 太小的 nodes\n",
    "nodes = [node for node in nodes if node[1] >= 10]\n",
    "print(nodes)\n",
    "print(len(nodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

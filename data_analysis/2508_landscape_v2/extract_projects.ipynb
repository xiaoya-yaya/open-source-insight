{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c947852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import clickhouse_connect\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "load_dotenv()\n",
    "clickhouse_host = os.getenv(\"CLICKHOUSE_HOST\")\n",
    "username = os.getenv(\"CLICKHOUSE_USER\")\n",
    "password = os.getenv(\"CLICKHOUSE_PASSWORD\")\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "client = clickhouse_connect.get_client(host=clickhouse_host, port=8123, username=username, password=password)\n",
    "\n",
    "headers = {\n",
    "  \"Authorization\": f\"token {github_token}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 v1 和 v2 的数据\n",
    "df_v1 = pd.read_csv('landscapev1.csv', encoding='utf-8-sig')\n",
    "df_v2 = pd.read_csv('landscapev2.csv', encoding='utf-8-sig')\n",
    "\n",
    "# 找出在 v1 中但不在 v2 中的 repo_id\n",
    "v1_repo_ids = set(df_v1['repo_id'])\n",
    "v2_repo_ids = set(df_v2['repo_id'])\n",
    "missing_repo_ids = v1_repo_ids - v2_repo_ids\n",
    "\n",
    "# 对于每个缺失的 repo_id，从 v1 中获取对应的数据并添加到 v2\n",
    "new_rows = []\n",
    "for repo_id in missing_repo_ids:\n",
    "    v1_row = df_v1[df_v1['repo_id'] == repo_id].iloc[0]\n",
    "    new_row = {\n",
    "        'repo_id': v1_row['repo_id'],\n",
    "        'repo_name': v1_row['repo_name'],\n",
    "        'classification': v1_row['classification']\n",
    "    }\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# 将新行添加到 v2 中\n",
    "if new_rows:\n",
    "    df_new = pd.DataFrame(new_rows)\n",
    "    df_v2 = pd.concat([df_v2, df_new], ignore_index=True)\n",
    "    \n",
    "# 保存更新后的 v2 数据\n",
    "df_v2.to_csv('landscape.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f05c7",
   "metadata": {},
   "source": [
    "## 获取 Landscape 所需的 GitHub 仓库信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Import requests for making API calls\n",
    "\n",
    "# List of repositories to fetch\n",
    "# repo_names = pd.read_csv('landscape.csv')['repo_name'].tolist()\n",
    "repo_names = ['openai/codex',\n",
    "'78/xiaozhi-esp32',\n",
    "'deepseek-ai/DeepEP',\n",
    "'THUDM/slime',\n",
    "'inclusionAI/AReaL']\n",
    "\n",
    "repo_data = []  # Initialize empty list for repo data\n",
    "# fetch stars, language and descripiton through repo_name\n",
    "def fetch_repo_info(repo_names, headers):\n",
    "  github_repo_url = \"https://api.github.com/repos/\"\n",
    "  openrank_repo_url = \"https://oss.open-digger.cn/github/{repo_name}/openrank.json\"\n",
    "  for repo_name in repo_names:\n",
    "    response = requests.get(github_repo_url + repo_name, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "      data = response.json()\n",
    "      repo_id = data['id']\n",
    "      stars = data['stargazers_count'] \n",
    "      forks = data['forks_count']\n",
    "      language = data['language']\n",
    "      created_at = data['created_at'].split(\"T\")[0]\n",
    "      description = data['description']\n",
    "      topics = ','.join(data.get(\"topics\", [])) # 将topics列表转换为逗号分隔的字符串\n",
    "      # avatar_url = data['owner']['avatar_url']\n",
    "      openrank_url = openrank_repo_url.format(repo_name=repo_name)\n",
    "      openrank_response = requests.get(openrank_url)\n",
    "      if openrank_response.status_code == 200:\n",
    "        openrank_json = openrank_response.json()\n",
    "        openrank = openrank_json.get(\"2025-07\")\n",
    "      else:\n",
    "        openrank = None\n",
    "      \n",
    "      repo_data.append({\n",
    "        'repo_id': repo_id,\n",
    "        'repo_name': repo_name,\n",
    "        'stars': stars,\n",
    "        'forks': forks, \n",
    "        'openrank_25': round(openrank) if openrank else None,\n",
    "        'language': language,\n",
    "        'created_at': created_at,\n",
    "        'description': description,\n",
    "        'topics': topics\n",
    "      })\n",
    "    else:\n",
    "      print(f\"Failed to fetch data for {repo_name}\")\n",
    "  \n",
    "  return repo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc59e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data = fetch_repo_info(repo_names, headers)\n",
    "\n",
    "# Save the repository data to CSV file\n",
    "repo_df = pd.DataFrame(repo_data)\n",
    "if not repo_df.empty:\n",
    "    repo_df.to_csv('repository_data.csv', index=False)\n",
    "    print(f\"Repository data saved to repository_data.csv\")\n",
    "else:\n",
    "    print(\"No repository data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70783f26",
   "metadata": {},
   "source": [
    "## 获取 OpenRank > 50 的项目列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe46615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 查询 github openrank>50 top 项目列表\n",
    "def execute_query_top_openrank(created_at='2025-01-01'):\n",
    "  sql_query_top_openrank = \"\"\"\n",
    "    SELECT\n",
    "        repo_id,\n",
    "        repo_name,\n",
    "        ROUND(AVG(openrank)) AS avg_openrank_25\n",
    "    FROM\n",
    "        opensource.global_openrank\n",
    "    WHERE\n",
    "        platform = 'GitHub' AND\n",
    "        created_at >= %s\n",
    "    GROUP BY\n",
    "        repo_id, repo_name\n",
    "    HAVING\n",
    "        avg_openrank_25 >= 30 and avg_openrank_25 < 50\n",
    "    ORDER BY\n",
    "        avg_openrank_25 DESC\n",
    "\n",
    "  \"\"\"\n",
    "  formatted_query = sql_query_top_openrank % (f\"'{created_at}'\")\n",
    "  results = client.query(formatted_query)\n",
    "  return results\n",
    "\n",
    "\n",
    "results_openrank = execute_query_top_openrank()\n",
    "print(f\"Found {len(results_openrank.result_rows)} repositories with high OpenRank scores\")\n",
    "\n",
    "# 提取项目名称到列表\n",
    "repo_names = [row[1] for row in results_openrank.result_rows]\n",
    "print(f\"First 5 repositories: {repo_names[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bee69c",
   "metadata": {},
   "source": [
    "## 添加项目基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_repo_info(repo_name, headers):\n",
    "    \n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Process and return the data\n",
    "        return {\n",
    "            \"id\": data[\"id\"],\n",
    "            \"repo_name\": data[\"full_name\"],\n",
    "            \"stargazers_count\": data[\"stargazers_count\"],\n",
    "            \"forks_count\": data[\"forks_count\"],\n",
    "            \"language\": data[\"language\"],\n",
    "            \"created_at\": data[\"created_at\"],\n",
    "            \"description\": data[\"description\"],\n",
    "            \"topics\": data.get(\"topics\", [])\n",
    "        }\n",
    "\n",
    "results = []\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  print(f\"Processing {repo_name}...\")\n",
    "  \n",
    "  # Get repo info from GitHub API\n",
    "  repo_info = get_repo_info(repo_name, headers)\n",
    "  \n",
    "  if repo_info:\n",
    "        \n",
    "    # Compile all information\n",
    "    result = {\n",
    "      \"repo_id\": repo_info[\"id\"],\n",
    "      \"repo_name\": repo_info[\"repo_name\"],\n",
    "      \"stars\": repo_info[\"stargazers_count\"],\n",
    "      \"forks\": repo_info[\"forks_count\"],\n",
    "      \"language\": repo_info[\"language\"],\n",
    "      \"created_at\": repo_info[\"created_at\"].split(\"T\")[0],\n",
    "      \"description\": repo_info[\"description\"],\n",
    "      \"topics\": \", \".join(repo_info.get(\"topics\", [])) if repo_info.get(\"topics\") else \"\"\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "  else:\n",
    "    print(f\"Failed to get info for {repo_name}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "columns_order = [\"repo_id\", \"repo_name\", \"stars\", \"forks\", \"language\", \"created_at\", \"description\", 'topics']\n",
    "df_results = df_results[columns_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9033b",
   "metadata": {},
   "source": [
    "## 添加 OpenRank 值并存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54772ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract repo_id and avg_openrank from the results_openrank\n",
    "repo_id_avg_openrank = {row[1]: (row[0], row[2]) for row in results_openrank.result_rows}\n",
    "\n",
    "# Initialize new columns with None values\n",
    "df_results['repo_id'] = None\n",
    "df_results['avg_openrank_25'] = None\n",
    "\n",
    "# Fill in the values for repo_id and avg_openrank_25\n",
    "for idx, repo_name in enumerate(df_results['repo_name']):\n",
    "  if repo_name in repo_id_avg_openrank:\n",
    "    df_results.at[idx, 'repo_id'] = repo_id_avg_openrank[repo_name][0]\n",
    "    df_results.at[idx, 'avg_openrank_25'] = repo_id_avg_openrank[repo_name][1]\n",
    "\n",
    "# Update columns order to include the new columns\n",
    "columns_order = columns_order + ['repo_id', 'avg_openrank_25']\n",
    "df_results_new = df_results[columns_order]\n",
    "\n",
    "csv_filename = f\"repo_info_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "df_results_new.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Successfully processed {len(results)} repositories\")\n",
    "print(f\"Results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3c3bd",
   "metadata": {},
   "source": [
    "## 通过 OSS 获取 OpenRank 当月值 和 Trend 并存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取 repository_data.csv 并获取 OpenRank 趋势数据\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "repo_names = [\"microsoft/graphrag\",\n",
    "\"letta-ai/letta\",\n",
    "\"FoundationAgents/MetaGPT\",\n",
    "\"Significant-Gravitas/AutoGPT\",\n",
    "\"eosphoros-ai/DB-GPT\",\n",
    "\"deepset-ai/haystack\",\n",
    "\"chatchat-space/Langchain-Chatchat\",\n",
    "\"openxla/xla\",\n",
    "\"searxng/searxng\",\n",
    "\"ItzCrazyKns/Perplexica\",\n",
    "\"zaidmukaddam/scira\",\n",
    "\"google/A2A\",\n",
    "\"ComposioHQ/composio\",\n",
    "\"songquanpeng/one-api\",\n",
    "\"SillyTavern/SillyTavern\",\n",
    "\"ChatGPTNextWeb/NextChat\",\n",
    "\"chatboxai/chatbox\",\n",
    "\"oobabooga/text-generation-webui\",\n",
    "\"AUTOMATIC1111/stable-diffusion-webui\",\n",
    "\"stackblitz/bolt.new\",\n",
    "\"Aider-AI/aider\",\n",
    "\"sourcegraph/cody\",\n",
    "\"TabbyML/tabby\",\n",
    "\"qodo-ai/pr-agent\",\n",
    "\"PrefectHQ/prefect\",\n",
    "\"Zipstack/unstract\",\n",
    "\"iterative/datachain\",\n",
    "\"dbt-labs/dbt-core\",\n",
    "\"Unstructured-IO/unstructured\",\n",
    "\"dask/dask\",\n",
    "\"open-compass/opencompass\",\n",
    "\"lm-sys/FastChat\",\n",
    "\"mannaandpoem/OpenManus\",\n",
    "\"camel-ai/owl\",\n",
    "\"mindverse/Second-Me\",\n",
    "\"NVIDIA/nccl\",\n",
    "\"triton-inference-server/server\",\n",
    "\"nomic-ai/gpt4all\",\n",
    "\"kserve/kserve\",\n",
    "\"kvcache-ai/Mooncake\",\n",
    "\"vllm-project/aibrix\",\n",
    "\"mlc-ai/mlc-llm\",\n",
    "\"bentoml/BentoML\",\n",
    "\"microsoft/onnxruntime\",\n",
    "\"kvcache-ai/ktransformers\",\n",
    "\"InternLM/lmdeploy\",\n",
    "\"huggingface/text-generation-inference\",\n",
    "\"deepflowio/deepflow\",\n",
    "\"flyteorg/flyte\",\n",
    "\"Netflix/metaflow\",\n",
    "\"zenml-io/zenml\",\n",
    "\"Farama-Foundation/Gymnasium\",\n",
    "\"tensorflow/tensorflow\",\n",
    "\"keras-team/keras\",\n",
    "\"hpcaitech/ColossalAI\",\n",
    "\"microsoft/OmniParser\",\n",
    "\"unitycatalog/unitycatalog\",\n",
    "\"elastic/elasticsearch\",\n",
    "\"opensearch-project/OpenSearch\",\n",
    "\"lancedb/lancedb\",\n",
    "\"pgvector/pgvector\"]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# 遍历每个仓库获取 OpenRank 数据\n",
    "for repo_name in repo_names:\n",
    "    url = f\"https://oss.open-digger.cn/github/{repo_name}/openrank.json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # 获取7月份的openrank值\n",
    "            july_openrank = None\n",
    "            if \"2024-07\" in data:\n",
    "                july_openrank = round(data[\"2025-07\"])\n",
    "            \n",
    "            # 提取2025年的月度数据\n",
    "            trends_2025 = []\n",
    "            for month in range(1, 13):\n",
    "                month_key = f\"2025-{month:02d}\"\n",
    "                if month_key in data:\n",
    "                    trends_2025.append(round(data[month_key]))\n",
    "            \n",
    "            # 保存结果\n",
    "            results.append({\n",
    "                'repo_name': repo_name,\n",
    "                'july_2025_openrank': july_openrank,\n",
    "                'trends_2025': str(trends_2025)\n",
    "            })\n",
    "            \n",
    "            print(f\"成功获取 {repo_name} 的OpenRank数据 - 7月值: {july_openrank}, 2025年趋势: {len(trends_2025)}个月\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"获取 {repo_name} 数据失败: {response.status_code}\")\n",
    "            results.append({\n",
    "                'repo_name': repo_name,\n",
    "                'july_2024_openrank': None,\n",
    "                'trends_2025': '[]'\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理 {repo_name} 时发生错误: {str(e)}\")\n",
    "        results.append({\n",
    "            'repo_name': repo_name,\n",
    "            'july_2024_openrank': None,\n",
    "            'trends_2025': '[]'\n",
    "        })\n",
    "\n",
    "# 创建DataFrame并保存数据\n",
    "df_openrank = pd.DataFrame(results)\n",
    "df_openrank.to_csv('repository_openrank_data.csv', index=False)\n",
    "print(f\"已完成所有{len(repo_names)}个仓库的OpenRank数据获取和保存\")\n",
    "print(f\"数据已保存到 repository_openrank_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5712a0",
   "metadata": {},
   "source": [
    "## 标注出已有的项目和相应的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files with explicit encoding\n",
    "landscape_full = pd.read_csv('landscape-full-08.csv')\n",
    "landscape527 = pd.read_csv('landscape527_full.csv')\n",
    "\n",
    "print(f\"Loaded landscape-full-08.csv with {landscape_full.shape[0]} entries\")\n",
    "print(f\"Loaded landscape527_full.csv with {landscape527.shape[0]} entries\")\n",
    "\n",
    "# Check the column names\n",
    "print(\"\\nColumns in landscape-full-08.csv:\")\n",
    "print(landscape_full.columns.tolist())\n",
    "print(\"\\nColumns in landscape527_full.csv:\")\n",
    "print(landscape527.columns.tolist())\n",
    "\n",
    "# Create a mapping from repo_name to classification in landscape527\n",
    "classification_map = {}\n",
    "for _, row in landscape527.iterrows():\n",
    "  if 'repo_name' in landscape527.columns and 'classification' in landscape527.columns:\n",
    "    repo_name = row['repo_name']\n",
    "    classification = row['classification']\n",
    "    if pd.notna(repo_name) and pd.notna(classification):\n",
    "      classification_map[repo_name] = classification\n",
    "\n",
    "# Update the llm field in landscape_full with classifications from landscape527\n",
    "matches = 0\n",
    "for idx, row in landscape_full.iterrows():\n",
    "  if row['repo_name'] in classification_map:\n",
    "    landscape_full.at[idx, 'llm'] = classification_map[row['repo_name']]\n",
    "    matches += 1\n",
    "\n",
    "# Save the updated dataframe\n",
    "output_file = 'landscape_full_updated.csv'\n",
    "landscape_full.to_csv(output_file, index=False)\n",
    "print(f\"Updated dataframe saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce6b5",
   "metadata": {},
   "source": [
    "## 获取 Star 数 Top 1K 的 Rust 项目信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# GitHub API endpoint\n",
    "api_url = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# 设置请求头,需要替换为你的GitHub token\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    # \"Authorization\": \"token YOUR_GITHUB_TOKEN\"  # 如果需要更高的API限制,请取消注释并填入token\n",
    "}\n",
    "\n",
    "# 搜索参数 - 专门搜索Rust语言的项目\n",
    "params = {\n",
    "    \"q\": \"language:rust\",  # 指定搜索Rust语言的仓库\n",
    "    \"sort\": \"stars\",       # 按star数排序\n",
    "    \"order\": \"desc\",       # 降序排列\n",
    "    \"per_page\": 100       # 每页100条结果\n",
    "}\n",
    "\n",
    "all_repos = []\n",
    "pages = 10  # 获取10页,总共1000个仓库\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    try:\n",
    "        params[\"page\"] = page\n",
    "        response = requests.get(api_url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            repos = data[\"items\"]\n",
    "            \n",
    "            for repo in repos:\n",
    "                repo_data = {\n",
    "                    \"repo_name\": f\"{repo['owner']['login']}/{repo['name']}\",\n",
    "                    \"stars\": repo[\"stargazers_count\"],\n",
    "                    \"forks\": repo[\"forks_count\"],\n",
    "                    \"created_at\": repo[\"created_at\"],\n",
    "                    \"description\": repo[\"description\"],\n",
    "                    \"topics\": \",\".join(repo.get(\"topics\", []))\n",
    "                }\n",
    "                all_repos.append(repo_data)\n",
    "            \n",
    "            print(f\"成功获取第 {page} 页数据，当前已获取 {len(all_repos)} 个仓库\")\n",
    "            \n",
    "            # 检查是否还有更多数据\n",
    "            if len(repos) < 100:\n",
    "                print(\"已获取所有可用数据\")\n",
    "                break\n",
    "                \n",
    "            sleep(2)  # 避免触发API限制\n",
    "            \n",
    "        else:\n",
    "            print(f\"获取第 {page} 页数据失败: {response.status_code}\")\n",
    "            if response.status_code == 403:\n",
    "                print(\"可能达到API访问限制，请稍后再试或使用GitHub Token\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"处理第 {page} 页时发生错误: {str(e)}\")\n",
    "        break\n",
    "\n",
    "# 转换为DataFrame并保存\n",
    "top_repos_df = pd.DataFrame(all_repos)\n",
    "output_file = \"top_rust_repos.csv\"\n",
    "top_repos_df.to_csv(output_file, index=False)\n",
    "print(f\"已保存 {len(all_repos)} 个Rust仓库信息到 {output_file}\")\n",
    "\n",
    "# 显示前10个仓库的基本信息\n",
    "print(\"\\n前10个最受欢迎的Rust仓库:\")\n",
    "print(top_repos_df[[\"repo_name\", \"stars\", \"forks\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e032f9",
   "metadata": {},
   "source": [
    "获取新增/拿掉的项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取两个CSV文件\n",
    "df_v1 = pd.read_csv('landscapev1.csv')\n",
    "df_v2 = pd.read_csv('landscapev2.csv')\n",
    "\n",
    "# 找出在v1中但不在v2中的repo_id\n",
    "removed_repos = set(df_v1['repo_id']) - set(df_v2['repo_id'])\n",
    "\n",
    "# 在v1中添加新列'removed'\n",
    "df_v1['removed'] = ''\n",
    "\n",
    "# 标记被移除的项目\n",
    "df_v1.loc[df_v1['repo_id'].isin(removed_repos), 'removed'] = 'x'\n",
    "\n",
    "# 保存更新后的v1文件\n",
    "df_v1.to_csv('landscapev1.csv', index=False)\n",
    "\n",
    "print(f\"已标记 {len(removed_repos)} 个在v2中被移除的项目\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78704a8",
   "metadata": {},
   "source": [
    "## 根据 Landscape2.0 中所有项目的 description，获得词云图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3acb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将所有描述文本合并成一个字符串\n",
    "text = \"\"\"Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search\tanns,cloud-native,diskann,distributed,embedding-database,embedding-similarity,embedding-store,faiss,golang,hnsw,image-search,llm,nearest-neighbor-search,rag,vector-database,vector-search,vector-similarity,vector-store\n",
    "Open-source search and retrieval database for AI applications.\tdocument-retrieval,embeddings,llms\n",
    "Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native database​.\tapproximate-nearest-neighbor-search,generative-search,grpc,hnsw,hybrid-search,image-search,information-retrieval,mlops,nearest-neighbor-search,neural-search,recommender-system,search-engine,semantic-search,semantic-search-engine,similarity-search,vector-database,vector-search,vector-search-engine,vectors,weaviate\n",
    "Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/\tai-search,ai-search-engine,embeddings-similarity,hnsw,image-search,knn-algorithm,machine-learning,mlops,nearest-neighbor-search,neural-network,neural-search,recommender-system,search,search-engine,search-engines,similarity-search,vector-database,vector-search,vector-search-engine\n",
    "Tensors and Dynamic neural networks in Python with strong GPU acceleration\tautograd,deep-learning,gpu,machine-learning,neural-network,numpy,python,tensor\n",
    "PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice （『飞桨』核心框架，深度学习&机器学习高性能单机、分布式训练和跨平台部署）\tdeep-learning,distributed-training,efficiency,machine-learning,neural-network,paddlepaddle,python,scalability\n",
    "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more\tjax\n",
    "verl: Volcano Engine Reinforcement Learning for LLMs\t\n",
    "Distributed RL System for LLM Reasoning\tllm,llm-reasoning,machine-learning-systems,mlsys,reinforcement-learning,rl\n",
    "slime is a LLM post-training framework aiming for RL Scaling.\t\n",
    "Full-stack framework for building Multi-Agent Systems with memory, knowledge and reasoning.\tagents,agi,ai,developer-tools,framework,python\n",
    "🐫 CAMEL: The first and the best multi-agent framework. Finding the Scaling Law of Agents. https://www.camel-ai.org\tagent,ai-societies,artificial-intelligence,communicative-ai,cooperative-ai,deep-learning,large-language-models,multi-agent-systems,natural-language-processing\n",
    "A lightweight, powerful framework for multi-agent workflows\tagents,ai,framework,llm,openai,python\n",
    "Autonomous agents for everyone\tagent,agentic,ai,autonomous,chatbot,crypto,discord,eliza,elizaos,framework,plugins,rag,slack,swarm,telegram\n",
    "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.\tagents,ai,ai-agents,aiagentframework,llms\n",
    "The open source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.\tagentops,agents,ai,ai-governance,apache-spark,evaluation,langchain,llm-evaluation,llmops,machine-learning,ml,mlflow,mlops,model-management,observability,open-source,openai,prompt-engineering\n",
    "🔥 1Panel provides an intuitive web interface and MCP Server to manage websites, files, containers, databases, and LLMs on a Linux server.\t1panel,cockpit,docker,docker-ui,lamp,linux,lnmp,ollama,webmin\n",
    "🪢 Open source LLM engineering platform: LLM Observability, metrics, evals, prompt management, playground, datasets. Integrates with OpenTelemetry, Langchain, OpenAI SDK, LiteLLM, and more. 🍊YC W23\tanalytics,autogen,evaluation,langchain,large-language-models,llama-index,llm,llm-evaluation,llm-observability,llmops,monitoring,observability,open-source,openai,playground,prompt-engineering,prompt-management,self-hosted,ycombinator\n",
    "The AI developer platform. Use Weights & Biases to train and fine-tune models, and manage models from experimentation to production.\tai,collaboration,data-science,data-versioning,deep-learning,experiment-track,hyperparameter-optimization,hyperparameter-search,hyperparameter-tuning,jax,keras,machine-learning,ml-platform,mlops,model-versioning,pytorch,reinforcement-learning,reproducibility,tensorflow\n",
    "Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.\tlangchain,llama-index,llm,llm-evaluation,llm-observability,llmops,open-source,openai,playground,prompt-engineering\n",
    "AI Observability & Evaluation\tagents,ai-monitoring,ai-observability,aiengineering,anthropic,datasets,evals,langchain,llamaindex,llm-eval,llm-evaluation,llmops,llms,openai,prompt-engineering,smolagents\n",
    "MLRun is an open source MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications.\tdata-engineering,data-science,experiment-tracking,kubernetes,machine-learning,mlops,mlops-workflow,model-serving,python,workflow\n",
    "Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.\tci,ci-cd,cicd,evaluation,evaluation-framework,llm,llm-eval,llm-evaluation,llm-evaluation-framework,llmops,pentesting,prompt-engineering,prompt-testing,prompts,rag,red-teaming,testing,vulnerability-scanners\n",
    "An open-source runtime for composable workflows. Great for AI agents and CI/CD.\tagents,ai,caching,ci-cd,containers,continuous-deployment,continuous-integration,dag,dagger,devops,docker,graphql,workflows\n",
    "A high-throughput and memory-efficient inference and serving engine for LLMs\tamd,cuda,deepseek,gpt,hpu,inference,inferentia,llama,llm,llm-serving,llmops,mlops,model-serving,pytorch,qwen,rocm,tpu,trainium,transformer,xpu\n",
    "SGLang is a fast serving framework for large language models and vision language models.\tblackwell,cuda,deepseek,deepseek-r1,deepseek-v3,inference,kimi,llama,llama3,llama4,llama5,llava,llm,llm-serving,moe,openai,pytorch,qwen3,transformer,vlm\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.\tblackwell,cuda,llm-serving,moe,pytorch\n",
    "LLM inference in C/C++\tggml\n",
    "OpenVINO™ is an open source toolkit for optimizing and deploying AI inference\tai,computer-vision,deep-learning,deploy-ai,diffusion-models,generative-ai,good-first-issue,inference,llm-inference,natural-language-processing,nlp,openvino,optimize-ai,performance-boost,recommendation-system,speech-recognition,stable-diffusion,transformers,yolo\n",
    "Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.\tdeepseek,gemma,gemma3,gemma3n,go,golang,llama,llama2,llama3,llava,llm,llms,mistral,ollama,phi4,qwen\n",
    "A Datacenter Scale Distributed Inference Serving Framework\t\n",
    "Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\tartificial-intelligence,chatglm,deployment,flan-t5,gemma,ggml,glm4,inference,llama,llama3,llamacpp,llm,machine-learning,mistral,openai-api,pytorch,qwen,vllm,whisper,wizardlm\n",
    "RamaLama is an open-source developer tool that simplifies the local serving of AI models from any source and facilitates their use for inference in production, all through the familiar language of containers.\tai,containers,cuda,hip,inference-server,intel,llamacpp,llm,podman,vllm\n",
    "Simple, scalable AI model deployment on GPU clusters\tascend,cuda,deepseek,distributed-inference,genai,heterogeneous-cluster,inference,llama,llamacpp,llm,llm-inference,llm-serving,local-ai,maas,metal,mindie,openai,qwen,rocm,vllm\n",
    "Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, GLM4.5, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025).\tdeepseek-r1,deploy,embedding,grpo,internvl,liger,llama,llama4,llm,lora,megatron,multimodal,omni,open-r1,peft,qwen2-vl,qwen3,qwen3-moe,rft,sft\n",
    "Fine-tuning & Reinforcement Learning for LLMs. 🦥 Train Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.\tagent,ai,deepseek,deepseek-r1,fine-tuning,gemma,gemma3,llama,llama3,llm,llms,lora,mistral,openai,pytorch,qwen,qwen3,text-to-speech,tts,unsloth\n",
    "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)\tagent,ai,deepseek,fine-tuning,gemma,gpt,instruction-tuning,large-language-models,llama,llama3,llm,lora,moe,nlp,peft,qlora,quantization,qwen,rlhf,transformers\n",
    "An MCP-based chatbot | 一个基于MCP的聊天机器人\tchatbot,esp32,mcp\n",
    "A generative world for general-purpose robotics & embodied AI learning.\t\n",
    "A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)\tasr,deeplearning,generative-ai,large-language-models,machine-translation,multimodal,neural-networks,speaker-diariazation,speaker-recognition,speech-synthesis,speech-translation,tts\n",
    "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\tbillion-parameters,compression,data-parallelism,deep-learning,gpu,inference,machine-learning,mixture-of-experts,model-parallelism,pipeline-parallelism,pytorch,trillion-parameters,zero\n",
    "Ongoing research training transformer models at scale\tlarge-language-models,model-para,transformers\n",
    "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.\tdata-science,deep-learning,deployment,distributed,hyperparameter-optimization,hyperparameter-search,large-language-models,llm,llm-inference,llm-serving,machine-learning,optimization,parallel,python,pytorch,ray,reinforcement-learning,rllib,serving,tensorflow\n",
    "Apache Spark - A unified analytics engine for large-scale data processing\tbig-data,java,jdbc,python,r,scala,spark,sql\n",
    "A Cloud Native Batch System (Project under CNCF)\tai,batch-systems,bigdata,gene,golang,hpc,kubernetes,machine-learning,serving,training\n",
    "Label Studio is a multi-type data labeling and annotation tool with standardized output format\tannotation,annotation-tool,annotations,boundingbox,computer-vision,data-labeling,dataset,datasets,deep-learning,image-annotation,image-classification,image-labeling,image-labelling-tool,label-studio,labeling,labeling-tool,mlops,semantic-segmentation,text-annotation,yolo\n",
    "Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.\tannotation,annotation-tool,annotations,boundingbox,computer-vision,computer-vision-annotation,dataset,deep-learning,image-annotation,image-classification,image-labeling,image-labelling-tool,imagenet,labeling,labeling-tool,object-detection,pytorch,semantic-segmentation,tensorflow,video-annotation\n",
    "AI + Data, online. https://vespa.ai\tai,big-data,cpp,java,machine-learning,search-engine,server,serving,serving-recommendation,tensorflow,vector-search,vespa\n",
    "Apache Airflow - A platform to programmatically author, schedule, and monitor workflows\tairflow,apache,apache-airflow,automation,dag,data-engineering,data-integration,data-orchestrator,data-pipelines,data-science,elt,etl,machine-learning,mlops,orchestration,python,scheduler,workflow,workflow-engine,workflow-orchestration\n",
    "The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.\tbigquery,change-data-capture,data,data-analysis,data-collection,data-engineering,data-integration,data-pipeline,elt,etl,java,mssql,mysql,pipeline,postgresql,python,redshift,s3,self-hosted,snowflake\n",
    "An orchestration platform for the development, production, and observation of data assets.\tanalytics,dagster,data-engineering,data-integration,data-orchestrator,data-pipelines,data-science,etl,metadata,mlops,orchestration,python,scheduler,workflow,workflow-automation\n",
    "Apache Iceberg\tapache,hacktoberfest,iceberg\n",
    "OpenMetadata is a unified metadata platform for data discovery, data observability, and data governance powered by a central metadata repository, in-depth column level lineage, and seamless team collaboration.\tdata-catalog,data-collaboration,data-contracts,data-discovery,data-governance,data-lineage,data-observability,data-profiling,data-quality,data-quality-checks,data-science,data-validation,datadiscovery,dataengineering,dataquality,dbt,metadata,metadata-management,snowflake\n",
    "The Metadata Platform for your Data and AI Stack\tdata-catalog,data-discovery,data-governance,datahub,metadata\n",
    "World's most powerful open data catalog for building a high-performance, geo-distributed and federated metadata lake.\tai-catalog,data-catalog,datalake,federated-query,lakehouse,metadata,metalake,model-catalog,opendatacatalog,skycomputing,stratosphere\n",
    "An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs\tacid,analytics,big-data,delta-lake,spark\n",
    "Apache Paimon is a lake format that enables building a Realtime Lakehouse Architecture with Flink and Spark for both streaming and batch operations.\tbig-data,data-ingestion,flink,paimon,real-time-analytics,spark,streaming-datalake,table-store\n",
    "Upserts, Deletes And Incremental Processing on Big Data.\tapacheflink,apachehudi,apachespark,bigdata,data-integration,datalake,hudi,incremental-processing,stream-processing\n",
    "🍒 Cherry Studio is a desktop client that supports for multiple LLM providers.\tagent,anthropic,assistant,chatbot,chatbotai,electron,llm,mcp-client,openai\n",
    "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)\tai,llm,llm-ui,llm-webui,llms,mcp,ollama,ollama-webui,open-webui,openai,openapi,rag,self-hosted,ui,webui\n",
    "🤯 Lobe Chat - an open-source, modern design AI chat framework. Supports multiple AI providers (OpenAI / Claude 4 / Gemini / DeepSeek / Ollama / Qwen), Knowledge Base (file upload / RAG ), one click install MCP Marketplace and Artifacts / Thinking. One-click FREE deployment of your private AI Agent application.\tagent,ai,artifacts,chat,chatgpt,claude,deepseek,deepseek-r1,function-calling,gemini,gpt,knowledge-base,mcp,nextjs,ollama,openai,rag\n",
    "Enhanced ChatGPT Clone: Features Agents, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-4o, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active project.\tai,anthropic,artifacts,aws,azure,chatgpt,chatgpt-clone,claude,clone,dall-e-3,deepseek,gemini,google,librechat,o1,openai,plugins,responses-api,vision,webui\n",
    "✨ 易上手的多平台 LLM 聊天机器人及开发框架 ✨ 支持 QQ、QQ频道、Telegram、企微、飞书、钉钉 | 知识库、MCP 服务器、OpenAI、DeepSeek、Gemini、硅基流动、月之暗面、Ollama、OneAPI、Dify\tagent,ai,chatbot,chatgpt,docker,gemini,gpt,llama,llm,mcp,openai,python,qq,qqbot,qqchannel,telegram\n",
    "A privacy-first, self-hosted, fully open source personal knowledge management software, written in typescript and golang.\tanki,chatgpt,deepseek,electron,evernote,knowledge-base,local-first,markdown,note-taking,notes-app,notion,obsidian,ocr,ollama,openai,pdf,s3,self-hosted,webdav\n",
    "Get your documents ready for gen AI\tai,convert,document-parser,document-parsing,documents,docx,html,markdown,pdf,pdf-converter,pdf-to-json,pdf-to-text,pptx,tables,xlsx\n",
    "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.\tai-agents,custom-ai-agents,deepseek,kimi,llama3,llm,lmstudio,local-llm,localai,mcp,mcp-servers,moonshot,multimodal,no-code,ollama,qwen3,rag,vector-database,web-scraping\n",
    "Streamlit — A faster way to build and share data apps.\tdata-analysis,data-science,data-visualization,deep-learning,developer-tools,machine-learning,python,streamlit\n",
    "Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!\tdata-analysis,data-science,data-visualization,deep-learning,deploy,gradio,gradio-interface,hacktoberfest,interface,machine-learning,models,python,python-notebook,ui,ui-components\n",
    "cuDF - GPU DataFrame Library\tarrow,cpp,cuda,cudf,dask,data-analysis,data-science,dataframe,gpu,pandas,pydata,python,rapids\n",
    "A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.\tcuda,deep-learning,fp8,gpu,jax,machine-learning,python,pytorch\n",
    "FlashInfer: Kernel Library for LLM Serving\tattention,cuda,distributed-inference,gpu,jit,large-large-models,llm-inference,moe,nvidia,pytorch\n",
    "CUDA Templates for Linear Algebra Subroutines\tcpp,cuda,deep-learning,deep-learning-library,gpu,nvidia\n",
    "MLX: An array framework for Apple silicon\tmlx\n",
    "DeepEP: an efficient expert-parallel communication library\t\n",
    "Fast and memory-efficient exact attention\t\n",
    "Development repository for the Triton language and compiler\t\n",
    "The Modular Platform (includes MAX & Mojo)\tai,language,machine-learning,max,modular,mojo,programming-language\n",
    "An open-source AI agent that brings the power of Gemini directly into your terminal.\tgemini,gemini-api\n",
    "AI coding agent, built for the terminal.\t\n",
    "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.\t\n",
    "an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM\t\n",
    "⏩ Create, share, and use custom AI code assistants with our open-source IDE extensions and hub of rules, tools, and models\tai,chatgpt,copilot,developer-tools,intellij,jetbrains,llm,open-source,openai,pycharm,software-development,visual-studio-code,vscode\n",
    "🙌 OpenHands: Code Less, Make More\tagent,artificial-intelligence,chatgpt,claude-ai,cli,developer-tools,gpt,llm,openai\n",
    "A reactive notebook for Python — run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. All in a modern, AI-native editor.\tartificial-intelligence,dag,data-science,data-visualization,dataflow,developer-tools,machine-learning,notebooks,pipeline,python,reactive,sql,web-app\n",
    "Lightweight coding agent that runs in your terminal\t\n",
    "Use your Neovim like using Cursor AI IDE!\t\n",
    "Production-ready platform for agentic workflow development.\tagent,agentic-ai,agentic-framework,agentic-workflow,ai,automation,gemini,genai,gpt,gpt-4,llm,low-code,mcp,nextjs,no-code,openai,orchestration,python,rag,workflow\n",
    "Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.\tai,apis,automation,cli,data-flow,development,integration-framework,integrations,ipaas,low-code,low-code-platform,mcp,mcp-client,mcp-server,n8n,no-code,self-hosted,typescript,workflow,workflow-automation\n",
    "The Postgres development platform. Supabase gives you a dedicated Postgres database to build your web, mobile, and AI applications.\tai,alternative,auth,database,deno,embeddings,example,firebase,nextjs,oauth2,pgvector,postgis,postgres,postgresql,postgrest,realtime,supabase,vectors,websockets\n",
    "RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.\tagent,agentic,agentic-ai,agentic-workflow,ai,ai-search,deep-learning,deep-research,deepseek,deepseek-r1,document-parser,document-understanding,graphrag,llm,mcp,multi-agent,ollama,openai,rag,retrieval-augmented-generation\n",
    "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.\tagents,chatgpt,generative-ai,large-language-models,multiagent,react-flow\n",
    "The TypeScript AI agent framework. ⚡ Assistants, RAG, observability. Supports any LLM: GPT-4, Claude, Gemini, Llama.\tagents,ai,chatbots,evals,javascript,llm,mcp,nextjs,nodejs,reactjs,tts,typescript,workflows\n",
    "AI Agents & MCPs & AI Workflow Automation • (280+ MCP servers for AI agents) • AI Automation / AI Agent with MCPs • AI Workflows & AI Agents • MCPs for AI Agents\tai-agent,ai-agent-tools,ai-agents,ai-agents-framework,mcp,mcp-server,mcp-tools,mcps,n8n-alternative,no-code-automation,workflow,workflow-automation,workflows\n",
    "🔥 MaxKB is an open-source platform for building enterprise-grade agents.  MaxKB 是强大易用的开源企业级智能体平台。\tagent,agentic-ai,chatbot,deepseek-r1,knowledgebase,langchain,llama3,llm,maxkb,mcp-server,ollama,pgvector,qwen3,rag\n",
    "FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.\tagent,claude,deepseek,llm,mcp,nextjs,openai,qwen,rag,workflow\n",
    "Build AI Agents, Visually\tagentic-ai,agentic-workflow,agents,artificial-intelligence,chatbot,chatgpt,javascript,langchain,large-language-models,low-code,multiagent-systems,no-code,openai,rag,react,typescript,workflow-automation\n",
    "Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]\tai-gateway,anthropic,azure-openai,bedrock,gateway,langchain,litellm,llm,llm-gateway,llmops,mcp-gateway,openai,openai-proxy,vertex-ai\n",
    "The AI Toolkit for TypeScript. From the creators of Next.js, the AI SDK is a free open-source library for building AI-powered applications and agents\tanthropic,artificial-intelligence,gemini,generative-ai,generative-ui,javascript,language-model,llm,nextjs,openai,react,svelte,typescript,vercel,vue\n",
    "The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.\tai,python,pytorch,stable-diffusion\n",
    "An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.\tagent,agentic,agentic-ai,agents,agents-sdk,ai,ai-agents,aiagentframework,genai,genai-chatbot,llm,llms,multi-agent,multi-agent-systems,multi-agents,multi-agents-collaboration\n",
    "🌐 Make websites accessible for AI agents. Automate tasks online with ease.\tai-agents,ai-tools,browser-automation,browser-use,llm,playwright,python\n",
    "Model Context Protocol Servers\t\n",
    "Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.\tagent,ai,aiagent,application,chatbots,chatgpt,embeddings,llm,long-term-memory,memory,memory-management,python,rag,state-management,vector-database\n",
    "Build resilient language agents as graphs.\t\n",
    "Agent Framework / shim to use Pydantic with LLMs\tagent-framework,llms,pydantic,python\n",
    "🦜🔗 Build context-aware reasoning applications\tai,anthropic,gemini,langchain,llm,openai,python\n",
    "A powerful framework for building realtime voice AI agents 🤖🎙️📹\tagents,ai,openai,real-time,video,voice\n",
    "An Application Framework for AI Engineering\tartificial-intelligence,java,spring-ai\n",
    "LlamaIndex is the leading framework for building LLM-powered agents over your data.\tagents,application,data,fine-tuning,framework,llamaindex,llm,multi-agents,rag,vector-database\n",
    "Integrate cutting-edge LLM technology quickly and easily into your apps\tai,artificial-intelligence,llm,openai,sdk\n",
    "Open Source framework for voice and multimodal conversational AI\tai,chatbot-framework,chatbots,real-time,voice,voice-assistant\n",
    "A programming framework for agentic AI 🤖 PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour\tagentic,agentic-agi,agents,ai,autogen,autogen-ecosystem,chatgpt,framework,llm-agent,llm-framework\"\"\"\n",
    "\n",
    "# 定义常用词列表\n",
    "common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                'of', 'with', 'by', 'from', 'up', 'about', 'into', 'over', 'after',\n",
    "                'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "                'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "                'this', 'that', 'these', 'those',\n",
    "                'your', 'my', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "# 创建一个字典来存储单词频率\n",
    "word_freq = {}\n",
    "\n",
    "# 第一步：按空格分割\n",
    "words = []\n",
    "for word in text.lower().split():\n",
    "    # 处理破折号分隔的单词\n",
    "    if '-' in word:\n",
    "        words.extend(word.split('-'))\n",
    "    else:\n",
    "        words.append(word)\n",
    "\n",
    "# 第二步：按逗号分割\n",
    "comma_words = []\n",
    "for word in words:\n",
    "    if ',' in word:\n",
    "        comma_words.extend(word.split(','))\n",
    "    else:\n",
    "        comma_words.append(word)\n",
    "\n",
    "# 更新words列表\n",
    "words = comma_words\n",
    "\n",
    "for word in words:\n",
    "    # 去除标点符号\n",
    "    word = word.strip('.,!?()[]{}\":;-/&')\n",
    "    # 确保单词不是空字符串且不是常用词\n",
    "    if word and word not in common_words:\n",
    "        # 将复数形式转为单数形式(简单的s结尾情况)\n",
    "        if word.endswith('s') and word[:-1] in word_freq:\n",
    "            base_word = word[:-1]\n",
    "            word_freq[base_word] = word_freq.get(base_word, 0) + 1\n",
    "        else:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0555d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 按频率降序排序\n",
    "sorted_word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"去除常用词后的词频统计结果（前100个）：\")\n",
    "for word, freq in list(sorted_word_freq.items())[:100]:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c947852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import clickhouse_connect\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "load_dotenv()\n",
    "clickhouse_host = os.getenv(\"CLICKHOUSE_HOST\")\n",
    "username = os.getenv(\"CLICKHOUSE_USER\")\n",
    "password = os.getenv(\"CLICKHOUSE_PASSWORD\")\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "client = clickhouse_connect.get_client(host=clickhouse_host, port=8123, username=username, password=password)\n",
    "\n",
    "headers = {\n",
    "  \"Authorization\": f\"token {github_token}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å– v1 å’Œ v2 çš„æ•°æ®\n",
    "df_v1 = pd.read_csv('landscapev1.csv', encoding='utf-8-sig')\n",
    "df_v2 = pd.read_csv('landscapev2.csv', encoding='utf-8-sig')\n",
    "\n",
    "# æ‰¾å‡ºåœ¨ v1 ä¸­ä½†ä¸åœ¨ v2 ä¸­çš„ repo_id\n",
    "v1_repo_ids = set(df_v1['repo_id'])\n",
    "v2_repo_ids = set(df_v2['repo_id'])\n",
    "missing_repo_ids = v1_repo_ids - v2_repo_ids\n",
    "\n",
    "# å¯¹äºæ¯ä¸ªç¼ºå¤±çš„ repo_idï¼Œä» v1 ä¸­è·å–å¯¹åº”çš„æ•°æ®å¹¶æ·»åŠ åˆ° v2\n",
    "new_rows = []\n",
    "for repo_id in missing_repo_ids:\n",
    "    v1_row = df_v1[df_v1['repo_id'] == repo_id].iloc[0]\n",
    "    new_row = {\n",
    "        'repo_id': v1_row['repo_id'],\n",
    "        'repo_name': v1_row['repo_name'],\n",
    "        'classification': v1_row['classification']\n",
    "    }\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# å°†æ–°è¡Œæ·»åŠ åˆ° v2 ä¸­\n",
    "if new_rows:\n",
    "    df_new = pd.DataFrame(new_rows)\n",
    "    df_v2 = pd.concat([df_v2, df_new], ignore_index=True)\n",
    "    \n",
    "# ä¿å­˜æ›´æ–°åçš„ v2 æ•°æ®\n",
    "df_v2.to_csv('landscape.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f05c7",
   "metadata": {},
   "source": [
    "## è·å– Landscape æ‰€éœ€çš„ GitHub ä»“åº“ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf35e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # Import requests for making API calls\n",
    "\n",
    "# List of repositories to fetch\n",
    "# repo_names = pd.read_csv('landscape.csv')['repo_name'].tolist()\n",
    "repo_names = ['openai/codex',\n",
    "'78/xiaozhi-esp32',\n",
    "'deepseek-ai/DeepEP',\n",
    "'THUDM/slime',\n",
    "'inclusionAI/AReaL']\n",
    "\n",
    "repo_data = []  # Initialize empty list for repo data\n",
    "# fetch stars, language and descripiton through repo_name\n",
    "def fetch_repo_info(repo_names, headers):\n",
    "  github_repo_url = \"https://api.github.com/repos/\"\n",
    "  openrank_repo_url = \"https://oss.open-digger.cn/github/{repo_name}/openrank.json\"\n",
    "  for repo_name in repo_names:\n",
    "    response = requests.get(github_repo_url + repo_name, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "      data = response.json()\n",
    "      repo_id = data['id']\n",
    "      stars = data['stargazers_count'] \n",
    "      forks = data['forks_count']\n",
    "      language = data['language']\n",
    "      created_at = data['created_at'].split(\"T\")[0]\n",
    "      description = data['description']\n",
    "      topics = ','.join(data.get(\"topics\", [])) # å°†topicsåˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²\n",
    "      # avatar_url = data['owner']['avatar_url']\n",
    "      openrank_url = openrank_repo_url.format(repo_name=repo_name)\n",
    "      openrank_response = requests.get(openrank_url)\n",
    "      if openrank_response.status_code == 200:\n",
    "        openrank_json = openrank_response.json()\n",
    "        openrank = openrank_json.get(\"2025-07\")\n",
    "      else:\n",
    "        openrank = None\n",
    "      \n",
    "      repo_data.append({\n",
    "        'repo_id': repo_id,\n",
    "        'repo_name': repo_name,\n",
    "        'stars': stars,\n",
    "        'forks': forks, \n",
    "        'openrank_25': round(openrank) if openrank else None,\n",
    "        'language': language,\n",
    "        'created_at': created_at,\n",
    "        'description': description,\n",
    "        'topics': topics\n",
    "      })\n",
    "    else:\n",
    "      print(f\"Failed to fetch data for {repo_name}\")\n",
    "  \n",
    "  return repo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc59e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_data = fetch_repo_info(repo_names, headers)\n",
    "\n",
    "# Save the repository data to CSV file\n",
    "repo_df = pd.DataFrame(repo_data)\n",
    "if not repo_df.empty:\n",
    "    repo_df.to_csv('repository_data.csv', index=False)\n",
    "    print(f\"Repository data saved to repository_data.csv\")\n",
    "else:\n",
    "    print(\"No repository data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70783f26",
   "metadata": {},
   "source": [
    "## è·å– OpenRank > 50 çš„é¡¹ç›®åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe46615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æŸ¥è¯¢ github openrank>50 top é¡¹ç›®åˆ—è¡¨\n",
    "def execute_query_top_openrank(created_at='2025-01-01'):\n",
    "  sql_query_top_openrank = \"\"\"\n",
    "    SELECT\n",
    "        repo_id,\n",
    "        repo_name,\n",
    "        ROUND(AVG(openrank)) AS avg_openrank_25\n",
    "    FROM\n",
    "        opensource.global_openrank\n",
    "    WHERE\n",
    "        platform = 'GitHub' AND\n",
    "        created_at >= %s\n",
    "    GROUP BY\n",
    "        repo_id, repo_name\n",
    "    HAVING\n",
    "        avg_openrank_25 >= 30 and avg_openrank_25 < 50\n",
    "    ORDER BY\n",
    "        avg_openrank_25 DESC\n",
    "\n",
    "  \"\"\"\n",
    "  formatted_query = sql_query_top_openrank % (f\"'{created_at}'\")\n",
    "  results = client.query(formatted_query)\n",
    "  return results\n",
    "\n",
    "\n",
    "results_openrank = execute_query_top_openrank()\n",
    "print(f\"Found {len(results_openrank.result_rows)} repositories with high OpenRank scores\")\n",
    "\n",
    "# æå–é¡¹ç›®åç§°åˆ°åˆ—è¡¨\n",
    "repo_names = [row[1] for row in results_openrank.result_rows]\n",
    "print(f\"First 5 repositories: {repo_names[:5]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bee69c",
   "metadata": {},
   "source": [
    "## æ·»åŠ é¡¹ç›®åŸºæœ¬ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a0e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_repo_info(repo_name, headers):\n",
    "    \n",
    "    url = f\"https://api.github.com/repos/{repo_name}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Process and return the data\n",
    "        return {\n",
    "            \"id\": data[\"id\"],\n",
    "            \"repo_name\": data[\"full_name\"],\n",
    "            \"stargazers_count\": data[\"stargazers_count\"],\n",
    "            \"forks_count\": data[\"forks_count\"],\n",
    "            \"language\": data[\"language\"],\n",
    "            \"created_at\": data[\"created_at\"],\n",
    "            \"description\": data[\"description\"],\n",
    "            \"topics\": data.get(\"topics\", [])\n",
    "        }\n",
    "\n",
    "results = []\n",
    "\n",
    "for repo_name in repo_names:\n",
    "  print(f\"Processing {repo_name}...\")\n",
    "  \n",
    "  # Get repo info from GitHub API\n",
    "  repo_info = get_repo_info(repo_name, headers)\n",
    "  \n",
    "  if repo_info:\n",
    "        \n",
    "    # Compile all information\n",
    "    result = {\n",
    "      \"repo_id\": repo_info[\"id\"],\n",
    "      \"repo_name\": repo_info[\"repo_name\"],\n",
    "      \"stars\": repo_info[\"stargazers_count\"],\n",
    "      \"forks\": repo_info[\"forks_count\"],\n",
    "      \"language\": repo_info[\"language\"],\n",
    "      \"created_at\": repo_info[\"created_at\"].split(\"T\")[0],\n",
    "      \"description\": repo_info[\"description\"],\n",
    "      \"topics\": \", \".join(repo_info.get(\"topics\", [])) if repo_info.get(\"topics\") else \"\"\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "  else:\n",
    "    print(f\"Failed to get info for {repo_name}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "columns_order = [\"repo_id\", \"repo_name\", \"stars\", \"forks\", \"language\", \"created_at\", \"description\", 'topics']\n",
    "df_results = df_results[columns_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9033b",
   "metadata": {},
   "source": [
    "## æ·»åŠ  OpenRank å€¼å¹¶å­˜å‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54772ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract repo_id and avg_openrank from the results_openrank\n",
    "repo_id_avg_openrank = {row[1]: (row[0], row[2]) for row in results_openrank.result_rows}\n",
    "\n",
    "# Initialize new columns with None values\n",
    "df_results['repo_id'] = None\n",
    "df_results['avg_openrank_25'] = None\n",
    "\n",
    "# Fill in the values for repo_id and avg_openrank_25\n",
    "for idx, repo_name in enumerate(df_results['repo_name']):\n",
    "  if repo_name in repo_id_avg_openrank:\n",
    "    df_results.at[idx, 'repo_id'] = repo_id_avg_openrank[repo_name][0]\n",
    "    df_results.at[idx, 'avg_openrank_25'] = repo_id_avg_openrank[repo_name][1]\n",
    "\n",
    "# Update columns order to include the new columns\n",
    "columns_order = columns_order + ['repo_id', 'avg_openrank_25']\n",
    "df_results_new = df_results[columns_order]\n",
    "\n",
    "csv_filename = f\"repo_info_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "df_results_new.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Successfully processed {len(results)} repositories\")\n",
    "print(f\"Results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d3c3bd",
   "metadata": {},
   "source": [
    "## é€šè¿‡ OSS è·å– OpenRank å½“æœˆå€¼ å’Œ Trend å¹¶å­˜å‚¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å– repository_data.csv å¹¶è·å– OpenRank è¶‹åŠ¿æ•°æ®\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "repo_names = [\"microsoft/graphrag\",\n",
    "\"letta-ai/letta\",\n",
    "\"FoundationAgents/MetaGPT\",\n",
    "\"Significant-Gravitas/AutoGPT\",\n",
    "\"eosphoros-ai/DB-GPT\",\n",
    "\"deepset-ai/haystack\",\n",
    "\"chatchat-space/Langchain-Chatchat\",\n",
    "\"openxla/xla\",\n",
    "\"searxng/searxng\",\n",
    "\"ItzCrazyKns/Perplexica\",\n",
    "\"zaidmukaddam/scira\",\n",
    "\"google/A2A\",\n",
    "\"ComposioHQ/composio\",\n",
    "\"songquanpeng/one-api\",\n",
    "\"SillyTavern/SillyTavern\",\n",
    "\"ChatGPTNextWeb/NextChat\",\n",
    "\"chatboxai/chatbox\",\n",
    "\"oobabooga/text-generation-webui\",\n",
    "\"AUTOMATIC1111/stable-diffusion-webui\",\n",
    "\"stackblitz/bolt.new\",\n",
    "\"Aider-AI/aider\",\n",
    "\"sourcegraph/cody\",\n",
    "\"TabbyML/tabby\",\n",
    "\"qodo-ai/pr-agent\",\n",
    "\"PrefectHQ/prefect\",\n",
    "\"Zipstack/unstract\",\n",
    "\"iterative/datachain\",\n",
    "\"dbt-labs/dbt-core\",\n",
    "\"Unstructured-IO/unstructured\",\n",
    "\"dask/dask\",\n",
    "\"open-compass/opencompass\",\n",
    "\"lm-sys/FastChat\",\n",
    "\"mannaandpoem/OpenManus\",\n",
    "\"camel-ai/owl\",\n",
    "\"mindverse/Second-Me\",\n",
    "\"NVIDIA/nccl\",\n",
    "\"triton-inference-server/server\",\n",
    "\"nomic-ai/gpt4all\",\n",
    "\"kserve/kserve\",\n",
    "\"kvcache-ai/Mooncake\",\n",
    "\"vllm-project/aibrix\",\n",
    "\"mlc-ai/mlc-llm\",\n",
    "\"bentoml/BentoML\",\n",
    "\"microsoft/onnxruntime\",\n",
    "\"kvcache-ai/ktransformers\",\n",
    "\"InternLM/lmdeploy\",\n",
    "\"huggingface/text-generation-inference\",\n",
    "\"deepflowio/deepflow\",\n",
    "\"flyteorg/flyte\",\n",
    "\"Netflix/metaflow\",\n",
    "\"zenml-io/zenml\",\n",
    "\"Farama-Foundation/Gymnasium\",\n",
    "\"tensorflow/tensorflow\",\n",
    "\"keras-team/keras\",\n",
    "\"hpcaitech/ColossalAI\",\n",
    "\"microsoft/OmniParser\",\n",
    "\"unitycatalog/unitycatalog\",\n",
    "\"elastic/elasticsearch\",\n",
    "\"opensearch-project/OpenSearch\",\n",
    "\"lancedb/lancedb\",\n",
    "\"pgvector/pgvector\"]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# éå†æ¯ä¸ªä»“åº“è·å– OpenRank æ•°æ®\n",
    "for repo_name in repo_names:\n",
    "    url = f\"https://oss.open-digger.cn/github/{repo_name}/openrank.json\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # è·å–7æœˆä»½çš„openrankå€¼\n",
    "            july_openrank = None\n",
    "            if \"2024-07\" in data:\n",
    "                july_openrank = round(data[\"2025-07\"])\n",
    "            \n",
    "            # æå–2025å¹´çš„æœˆåº¦æ•°æ®\n",
    "            trends_2025 = []\n",
    "            for month in range(1, 13):\n",
    "                month_key = f\"2025-{month:02d}\"\n",
    "                if month_key in data:\n",
    "                    trends_2025.append(round(data[month_key]))\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            results.append({\n",
    "                'repo_name': repo_name,\n",
    "                'july_2025_openrank': july_openrank,\n",
    "                'trends_2025': str(trends_2025)\n",
    "            })\n",
    "            \n",
    "            print(f\"æˆåŠŸè·å– {repo_name} çš„OpenRankæ•°æ® - 7æœˆå€¼: {july_openrank}, 2025å¹´è¶‹åŠ¿: {len(trends_2025)}ä¸ªæœˆ\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"è·å– {repo_name} æ•°æ®å¤±è´¥: {response.status_code}\")\n",
    "            results.append({\n",
    "                'repo_name': repo_name,\n",
    "                'july_2024_openrank': None,\n",
    "                'trends_2025': '[]'\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç† {repo_name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "        results.append({\n",
    "            'repo_name': repo_name,\n",
    "            'july_2024_openrank': None,\n",
    "            'trends_2025': '[]'\n",
    "        })\n",
    "\n",
    "# åˆ›å»ºDataFrameå¹¶ä¿å­˜æ•°æ®\n",
    "df_openrank = pd.DataFrame(results)\n",
    "df_openrank.to_csv('repository_openrank_data.csv', index=False)\n",
    "print(f\"å·²å®Œæˆæ‰€æœ‰{len(repo_names)}ä¸ªä»“åº“çš„OpenRankæ•°æ®è·å–å’Œä¿å­˜\")\n",
    "print(f\"æ•°æ®å·²ä¿å­˜åˆ° repository_openrank_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5712a0",
   "metadata": {},
   "source": [
    "## æ ‡æ³¨å‡ºå·²æœ‰çš„é¡¹ç›®å’Œç›¸åº”çš„åˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the files with explicit encoding\n",
    "landscape_full = pd.read_csv('landscape-full-08.csv')\n",
    "landscape527 = pd.read_csv('landscape527_full.csv')\n",
    "\n",
    "print(f\"Loaded landscape-full-08.csv with {landscape_full.shape[0]} entries\")\n",
    "print(f\"Loaded landscape527_full.csv with {landscape527.shape[0]} entries\")\n",
    "\n",
    "# Check the column names\n",
    "print(\"\\nColumns in landscape-full-08.csv:\")\n",
    "print(landscape_full.columns.tolist())\n",
    "print(\"\\nColumns in landscape527_full.csv:\")\n",
    "print(landscape527.columns.tolist())\n",
    "\n",
    "# Create a mapping from repo_name to classification in landscape527\n",
    "classification_map = {}\n",
    "for _, row in landscape527.iterrows():\n",
    "  if 'repo_name' in landscape527.columns and 'classification' in landscape527.columns:\n",
    "    repo_name = row['repo_name']\n",
    "    classification = row['classification']\n",
    "    if pd.notna(repo_name) and pd.notna(classification):\n",
    "      classification_map[repo_name] = classification\n",
    "\n",
    "# Update the llm field in landscape_full with classifications from landscape527\n",
    "matches = 0\n",
    "for idx, row in landscape_full.iterrows():\n",
    "  if row['repo_name'] in classification_map:\n",
    "    landscape_full.at[idx, 'llm'] = classification_map[row['repo_name']]\n",
    "    matches += 1\n",
    "\n",
    "# Save the updated dataframe\n",
    "output_file = 'landscape_full_updated.csv'\n",
    "landscape_full.to_csv(output_file, index=False)\n",
    "print(f\"Updated dataframe saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ce6b5",
   "metadata": {},
   "source": [
    "## è·å– Star æ•° Top 1K çš„ Rust é¡¹ç›®ä¿¡æ¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# GitHub API endpoint\n",
    "api_url = \"https://api.github.com/search/repositories\"\n",
    "\n",
    "# è®¾ç½®è¯·æ±‚å¤´,éœ€è¦æ›¿æ¢ä¸ºä½ çš„GitHub token\n",
    "headers = {\n",
    "    \"Accept\": \"application/vnd.github.v3+json\",\n",
    "    # \"Authorization\": \"token YOUR_GITHUB_TOKEN\"  # å¦‚æœéœ€è¦æ›´é«˜çš„APIé™åˆ¶,è¯·å–æ¶ˆæ³¨é‡Šå¹¶å¡«å…¥token\n",
    "}\n",
    "\n",
    "# æœç´¢å‚æ•° - ä¸“é—¨æœç´¢Rustè¯­è¨€çš„é¡¹ç›®\n",
    "params = {\n",
    "    \"q\": \"language:rust\",  # æŒ‡å®šæœç´¢Rustè¯­è¨€çš„ä»“åº“\n",
    "    \"sort\": \"stars\",       # æŒ‰staræ•°æ’åº\n",
    "    \"order\": \"desc\",       # é™åºæ’åˆ—\n",
    "    \"per_page\": 100       # æ¯é¡µ100æ¡ç»“æœ\n",
    "}\n",
    "\n",
    "all_repos = []\n",
    "pages = 10  # è·å–10é¡µ,æ€»å…±1000ä¸ªä»“åº“\n",
    "\n",
    "for page in range(1, pages + 1):\n",
    "    try:\n",
    "        params[\"page\"] = page\n",
    "        response = requests.get(api_url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            repos = data[\"items\"]\n",
    "            \n",
    "            for repo in repos:\n",
    "                repo_data = {\n",
    "                    \"repo_name\": f\"{repo['owner']['login']}/{repo['name']}\",\n",
    "                    \"stars\": repo[\"stargazers_count\"],\n",
    "                    \"forks\": repo[\"forks_count\"],\n",
    "                    \"created_at\": repo[\"created_at\"],\n",
    "                    \"description\": repo[\"description\"],\n",
    "                    \"topics\": \",\".join(repo.get(\"topics\", []))\n",
    "                }\n",
    "                all_repos.append(repo_data)\n",
    "            \n",
    "            print(f\"æˆåŠŸè·å–ç¬¬ {page} é¡µæ•°æ®ï¼Œå½“å‰å·²è·å– {len(all_repos)} ä¸ªä»“åº“\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®\n",
    "            if len(repos) < 100:\n",
    "                print(\"å·²è·å–æ‰€æœ‰å¯ç”¨æ•°æ®\")\n",
    "                break\n",
    "                \n",
    "            sleep(2)  # é¿å…è§¦å‘APIé™åˆ¶\n",
    "            \n",
    "        else:\n",
    "            print(f\"è·å–ç¬¬ {page} é¡µæ•°æ®å¤±è´¥: {response.status_code}\")\n",
    "            if response.status_code == 403:\n",
    "                print(\"å¯èƒ½è¾¾åˆ°APIè®¿é—®é™åˆ¶ï¼Œè¯·ç¨åå†è¯•æˆ–ä½¿ç”¨GitHub Token\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†ç¬¬ {page} é¡µæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\")\n",
    "        break\n",
    "\n",
    "# è½¬æ¢ä¸ºDataFrameå¹¶ä¿å­˜\n",
    "top_repos_df = pd.DataFrame(all_repos)\n",
    "output_file = \"top_rust_repos.csv\"\n",
    "top_repos_df.to_csv(output_file, index=False)\n",
    "print(f\"å·²ä¿å­˜ {len(all_repos)} ä¸ªRustä»“åº“ä¿¡æ¯åˆ° {output_file}\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰10ä¸ªä»“åº“çš„åŸºæœ¬ä¿¡æ¯\n",
    "print(\"\\nå‰10ä¸ªæœ€å—æ¬¢è¿çš„Rustä»“åº“:\")\n",
    "print(top_repos_df[[\"repo_name\", \"stars\", \"forks\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e032f9",
   "metadata": {},
   "source": [
    "è·å–æ–°å¢/æ‹¿æ‰çš„é¡¹ç›®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15db5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–ä¸¤ä¸ªCSVæ–‡ä»¶\n",
    "df_v1 = pd.read_csv('landscapev1.csv')\n",
    "df_v2 = pd.read_csv('landscapev2.csv')\n",
    "\n",
    "# æ‰¾å‡ºåœ¨v1ä¸­ä½†ä¸åœ¨v2ä¸­çš„repo_id\n",
    "removed_repos = set(df_v1['repo_id']) - set(df_v2['repo_id'])\n",
    "\n",
    "# åœ¨v1ä¸­æ·»åŠ æ–°åˆ—'removed'\n",
    "df_v1['removed'] = ''\n",
    "\n",
    "# æ ‡è®°è¢«ç§»é™¤çš„é¡¹ç›®\n",
    "df_v1.loc[df_v1['repo_id'].isin(removed_repos), 'removed'] = 'x'\n",
    "\n",
    "# ä¿å­˜æ›´æ–°åçš„v1æ–‡ä»¶\n",
    "df_v1.to_csv('landscapev1.csv', index=False)\n",
    "\n",
    "print(f\"å·²æ ‡è®° {len(removed_repos)} ä¸ªåœ¨v2ä¸­è¢«ç§»é™¤çš„é¡¹ç›®\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78704a8",
   "metadata": {},
   "source": [
    "## æ ¹æ® Landscape2.0 ä¸­æ‰€æœ‰é¡¹ç›®çš„ descriptionï¼Œè·å¾—è¯äº‘å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3acb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# å°†æ‰€æœ‰æè¿°æ–‡æœ¬åˆå¹¶æˆä¸€ä¸ªå­—ç¬¦ä¸²\n",
    "text = \"\"\"Milvus is a high-performance, cloud-native vector database built for scalable vector ANN search\tanns,cloud-native,diskann,distributed,embedding-database,embedding-similarity,embedding-store,faiss,golang,hnsw,image-search,llm,nearest-neighbor-search,rag,vector-database,vector-search,vector-similarity,vector-store\n",
    "Open-source search and retrieval database for AI applications.\tdocument-retrieval,embeddings,llms\n",
    "Weaviate is an open-source vector database that stores both objects and vectors, allowing for the combination of vector search with structured filtering with the fault tolerance and scalability of a cloud-native databaseâ€‹.\tapproximate-nearest-neighbor-search,generative-search,grpc,hnsw,hybrid-search,image-search,information-retrieval,mlops,nearest-neighbor-search,neural-search,recommender-system,search-engine,semantic-search,semantic-search-engine,similarity-search,vector-database,vector-search,vector-search-engine,vectors,weaviate\n",
    "Qdrant - High-performance, massive-scale Vector Database and Vector Search Engine for the next generation of AI. Also available in the cloud https://cloud.qdrant.io/\tai-search,ai-search-engine,embeddings-similarity,hnsw,image-search,knn-algorithm,machine-learning,mlops,nearest-neighbor-search,neural-network,neural-search,recommender-system,search,search-engine,search-engines,similarity-search,vector-database,vector-search,vector-search-engine\n",
    "Tensors and Dynamic neural networks in Python with strong GPU acceleration\tautograd,deep-learning,gpu,machine-learning,neural-network,numpy,python,tensor\n",
    "PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice ï¼ˆã€é£æ¡¨ã€æ ¸å¿ƒæ¡†æ¶ï¼Œæ·±åº¦å­¦ä¹ &æœºå™¨å­¦ä¹ é«˜æ€§èƒ½å•æœºã€åˆ†å¸ƒå¼è®­ç»ƒå’Œè·¨å¹³å°éƒ¨ç½²ï¼‰\tdeep-learning,distributed-training,efficiency,machine-learning,neural-network,paddlepaddle,python,scalability\n",
    "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more\tjax\n",
    "verl: Volcano Engine Reinforcement Learning for LLMs\t\n",
    "Distributed RL System for LLM Reasoning\tllm,llm-reasoning,machine-learning-systems,mlsys,reinforcement-learning,rl\n",
    "slime is a LLM post-training framework aiming for RL Scaling.\t\n",
    "Full-stack framework for building Multi-Agent Systems with memory, knowledge and reasoning.\tagents,agi,ai,developer-tools,framework,python\n",
    "ğŸ« CAMEL: The first and the best multi-agent framework. Finding the Scaling Law of Agents. https://www.camel-ai.org\tagent,ai-societies,artificial-intelligence,communicative-ai,cooperative-ai,deep-learning,large-language-models,multi-agent-systems,natural-language-processing\n",
    "A lightweight, powerful framework for multi-agent workflows\tagents,ai,framework,llm,openai,python\n",
    "Autonomous agents for everyone\tagent,agentic,ai,autonomous,chatbot,crypto,discord,eliza,elizaos,framework,plugins,rag,slack,swarm,telegram\n",
    "Framework for orchestrating role-playing, autonomous AI agents. By fostering collaborative intelligence, CrewAI empowers agents to work together seamlessly, tackling complex tasks.\tagents,ai,ai-agents,aiagentframework,llms\n",
    "The open source developer platform to build AI/LLM applications and models with confidence. Enhance your AI applications with end-to-end tracking, observability, and evaluations, all in one integrated platform.\tagentops,agents,ai,ai-governance,apache-spark,evaluation,langchain,llm-evaluation,llmops,machine-learning,ml,mlflow,mlops,model-management,observability,open-source,openai,prompt-engineering\n",
    "ğŸ”¥ 1Panel provides an intuitive web interface and MCP Server to manage websites, files, containers, databases, and LLMs on a Linux server.\t1panel,cockpit,docker,docker-ui,lamp,linux,lnmp,ollama,webmin\n",
    "ğŸª¢ Open source LLM engineering platform: LLM Observability, metrics, evals, prompt management, playground, datasets. Integrates with OpenTelemetry, Langchain, OpenAI SDK, LiteLLM, and more. ğŸŠYC W23\tanalytics,autogen,evaluation,langchain,large-language-models,llama-index,llm,llm-evaluation,llm-observability,llmops,monitoring,observability,open-source,openai,playground,prompt-engineering,prompt-management,self-hosted,ycombinator\n",
    "The AI developer platform. Use Weights & Biases to train and fine-tune models, and manage models from experimentation to production.\tai,collaboration,data-science,data-versioning,deep-learning,experiment-track,hyperparameter-optimization,hyperparameter-search,hyperparameter-tuning,jax,keras,machine-learning,ml-platform,mlops,model-versioning,pytorch,reinforcement-learning,reproducibility,tensorflow\n",
    "Debug, evaluate, and monitor your LLM applications, RAG systems, and agentic workflows with comprehensive tracing, automated evaluations, and production-ready dashboards.\tlangchain,llama-index,llm,llm-evaluation,llm-observability,llmops,open-source,openai,playground,prompt-engineering\n",
    "AI Observability & Evaluation\tagents,ai-monitoring,ai-observability,aiengineering,anthropic,datasets,evals,langchain,llamaindex,llm-eval,llm-evaluation,llmops,llms,openai,prompt-engineering,smolagents\n",
    "MLRun is an open source MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications.\tdata-engineering,data-science,experiment-tracking,kubernetes,machine-learning,mlops,mlops-workflow,model-serving,python,workflow\n",
    "Test your prompts, agents, and RAGs. AI Red teaming, pentesting, and vulnerability scanning for LLMs. Compare performance of GPT, Claude, Gemini, Llama, and more. Simple declarative configs with command line and CI/CD integration.\tci,ci-cd,cicd,evaluation,evaluation-framework,llm,llm-eval,llm-evaluation,llm-evaluation-framework,llmops,pentesting,prompt-engineering,prompt-testing,prompts,rag,red-teaming,testing,vulnerability-scanners\n",
    "An open-source runtime for composable workflows. Great for AI agents and CI/CD.\tagents,ai,caching,ci-cd,containers,continuous-deployment,continuous-integration,dag,dagger,devops,docker,graphql,workflows\n",
    "A high-throughput and memory-efficient inference and serving engine for LLMs\tamd,cuda,deepseek,gpt,hpu,inference,inferentia,llama,llm,llm-serving,llmops,mlops,model-serving,pytorch,qwen,rocm,tpu,trainium,transformer,xpu\n",
    "SGLang is a fast serving framework for large language models and vision language models.\tblackwell,cuda,deepseek,deepseek-r1,deepseek-v3,inference,kimi,llama,llama3,llama4,llama5,llava,llm,llm-serving,moe,openai,pytorch,qwen3,transformer,vlm\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and support state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in performant way.\tblackwell,cuda,llm-serving,moe,pytorch\n",
    "LLM inference in C/C++\tggml\n",
    "OpenVINOâ„¢ is an open source toolkit for optimizing and deploying AI inference\tai,computer-vision,deep-learning,deploy-ai,diffusion-models,generative-ai,good-first-issue,inference,llm-inference,natural-language-processing,nlp,openvino,optimize-ai,performance-boost,recommendation-system,speech-recognition,stable-diffusion,transformers,yolo\n",
    "Get up and running with Llama 3.3, DeepSeek-R1, Phi-4, Gemma 3, Mistral Small 3.1 and other large language models.\tdeepseek,gemma,gemma3,gemma3n,go,golang,llama,llama2,llama3,llava,llm,llms,mistral,ollama,phi4,qwen\n",
    "A Datacenter Scale Distributed Inference Serving Framework\t\n",
    "Replace OpenAI GPT with another LLM in your app by changing a single line of code. Xinference gives you the freedom to use any LLM you need. With Xinference, you're empowered to run inference with any open-source language models, speech recognition models, and multimodal models, whether in the cloud, on-premises, or even on your laptop.\tartificial-intelligence,chatglm,deployment,flan-t5,gemma,ggml,glm4,inference,llama,llama3,llamacpp,llm,machine-learning,mistral,openai-api,pytorch,qwen,vllm,whisper,wizardlm\n",
    "RamaLama is an open-source developer tool that simplifies the local serving of AI models from any source and facilitates their use for inference in production, all through the familiar language of containers.\tai,containers,cuda,hip,inference-server,intel,llamacpp,llm,podman,vllm\n",
    "Simple, scalable AI model deployment on GPU clusters\tascend,cuda,deepseek,distributed-inference,genai,heterogeneous-cluster,inference,llama,llamacpp,llm,llm-inference,llm-serving,local-ai,maas,metal,mindie,openai,qwen,rocm,vllm\n",
    "Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, GLM4.5, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025).\tdeepseek-r1,deploy,embedding,grpo,internvl,liger,llama,llama4,llm,lora,megatron,multimodal,omni,open-r1,peft,qwen2-vl,qwen3,qwen3-moe,rft,sft\n",
    "Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.\tagent,ai,deepseek,deepseek-r1,fine-tuning,gemma,gemma3,llama,llama3,llm,llms,lora,mistral,openai,pytorch,qwen,qwen3,text-to-speech,tts,unsloth\n",
    "Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)\tagent,ai,deepseek,fine-tuning,gemma,gpt,instruction-tuning,large-language-models,llama,llama3,llm,lora,moe,nlp,peft,qlora,quantization,qwen,rlhf,transformers\n",
    "An MCP-based chatbot | ä¸€ä¸ªåŸºäºMCPçš„èŠå¤©æœºå™¨äºº\tchatbot,esp32,mcp\n",
    "A generative world for general-purpose robotics & embodied AI learning.\t\n",
    "A scalable generative AI framework built for researchers and developers working on Large Language Models, Multimodal, and Speech AI (Automatic Speech Recognition and Text-to-Speech)\tasr,deeplearning,generative-ai,large-language-models,machine-translation,multimodal,neural-networks,speaker-diariazation,speaker-recognition,speech-synthesis,speech-translation,tts\n",
    "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.\tbillion-parameters,compression,data-parallelism,deep-learning,gpu,inference,machine-learning,mixture-of-experts,model-parallelism,pipeline-parallelism,pytorch,trillion-parameters,zero\n",
    "Ongoing research training transformer models at scale\tlarge-language-models,model-para,transformers\n",
    "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.\tdata-science,deep-learning,deployment,distributed,hyperparameter-optimization,hyperparameter-search,large-language-models,llm,llm-inference,llm-serving,machine-learning,optimization,parallel,python,pytorch,ray,reinforcement-learning,rllib,serving,tensorflow\n",
    "Apache Spark - A unified analytics engine for large-scale data processing\tbig-data,java,jdbc,python,r,scala,spark,sql\n",
    "A Cloud Native Batch System (Project under CNCF)\tai,batch-systems,bigdata,gene,golang,hpc,kubernetes,machine-learning,serving,training\n",
    "Label Studio is a multi-type data labeling and annotation tool with standardized output format\tannotation,annotation-tool,annotations,boundingbox,computer-vision,data-labeling,dataset,datasets,deep-learning,image-annotation,image-classification,image-labeling,image-labelling-tool,label-studio,labeling,labeling-tool,mlops,semantic-segmentation,text-annotation,yolo\n",
    "Annotate better with CVAT, the industry-leading data engine for machine learning. Used and trusted by teams at any scale, for data of any scale.\tannotation,annotation-tool,annotations,boundingbox,computer-vision,computer-vision-annotation,dataset,deep-learning,image-annotation,image-classification,image-labeling,image-labelling-tool,imagenet,labeling,labeling-tool,object-detection,pytorch,semantic-segmentation,tensorflow,video-annotation\n",
    "AI + Data, online. https://vespa.ai\tai,big-data,cpp,java,machine-learning,search-engine,server,serving,serving-recommendation,tensorflow,vector-search,vespa\n",
    "Apache Airflow - A platform to programmatically author, schedule, and monitor workflows\tairflow,apache,apache-airflow,automation,dag,data-engineering,data-integration,data-orchestrator,data-pipelines,data-science,elt,etl,machine-learning,mlops,orchestration,python,scheduler,workflow,workflow-engine,workflow-orchestration\n",
    "The leading data integration platform for ETL / ELT data pipelines from APIs, databases & files to data warehouses, data lakes & data lakehouses. Both self-hosted and Cloud-hosted.\tbigquery,change-data-capture,data,data-analysis,data-collection,data-engineering,data-integration,data-pipeline,elt,etl,java,mssql,mysql,pipeline,postgresql,python,redshift,s3,self-hosted,snowflake\n",
    "An orchestration platform for the development, production, and observation of data assets.\tanalytics,dagster,data-engineering,data-integration,data-orchestrator,data-pipelines,data-science,etl,metadata,mlops,orchestration,python,scheduler,workflow,workflow-automation\n",
    "Apache Iceberg\tapache,hacktoberfest,iceberg\n",
    "OpenMetadata is a unified metadata platform for data discovery, data observability, and data governance powered by a central metadata repository, in-depth column level lineage, and seamless team collaboration.\tdata-catalog,data-collaboration,data-contracts,data-discovery,data-governance,data-lineage,data-observability,data-profiling,data-quality,data-quality-checks,data-science,data-validation,datadiscovery,dataengineering,dataquality,dbt,metadata,metadata-management,snowflake\n",
    "The Metadata Platform for your Data and AI Stack\tdata-catalog,data-discovery,data-governance,datahub,metadata\n",
    "World's most powerful open data catalog for building a high-performance, geo-distributed and federated metadata lake.\tai-catalog,data-catalog,datalake,federated-query,lakehouse,metadata,metalake,model-catalog,opendatacatalog,skycomputing,stratosphere\n",
    "An open-source storage framework that enables building a Lakehouse architecture with compute engines including Spark, PrestoDB, Flink, Trino, and Hive and APIs\tacid,analytics,big-data,delta-lake,spark\n",
    "Apache Paimon is a lake format that enables building a Realtime Lakehouse Architecture with Flink and Spark for both streaming and batch operations.\tbig-data,data-ingestion,flink,paimon,real-time-analytics,spark,streaming-datalake,table-store\n",
    "Upserts, Deletes And Incremental Processing on Big Data.\tapacheflink,apachehudi,apachespark,bigdata,data-integration,datalake,hudi,incremental-processing,stream-processing\n",
    "ğŸ’ Cherry Studio is a desktop client that supports for multiple LLM providers.\tagent,anthropic,assistant,chatbot,chatbotai,electron,llm,mcp-client,openai\n",
    "User-friendly AI Interface (Supports Ollama, OpenAI API, ...)\tai,llm,llm-ui,llm-webui,llms,mcp,ollama,ollama-webui,open-webui,openai,openapi,rag,self-hosted,ui,webui\n",
    "ğŸ¤¯ Lobe Chat - an open-source, modern design AI chat framework. Supports multiple AI providers (OpenAI / Claude 4 / Gemini / DeepSeek / Ollama / Qwen), Knowledge Base (file upload / RAG ), one click install MCP Marketplace and Artifacts / Thinking. One-click FREE deployment of your private AI Agent application.\tagent,ai,artifacts,chat,chatgpt,claude,deepseek,deepseek-r1,function-calling,gemini,gpt,knowledge-base,mcp,nextjs,ollama,openai,rag\n",
    "Enhanced ChatGPT Clone: Features Agents, DeepSeek, Anthropic, AWS, OpenAI, Responses API, Azure, Groq, o1, GPT-4o, Mistral, OpenRouter, Vertex AI, Gemini, Artifacts, AI model switching, message search, Code Interpreter, langchain, DALL-E-3, OpenAPI Actions, Functions, Secure Multi-User Auth, Presets, open-source for self-hosting. Active project.\tai,anthropic,artifacts,aws,azure,chatgpt,chatgpt-clone,claude,clone,dall-e-3,deepseek,gemini,google,librechat,o1,openai,plugins,responses-api,vision,webui\n",
    "âœ¨ æ˜“ä¸Šæ‰‹çš„å¤šå¹³å° LLM èŠå¤©æœºå™¨äººåŠå¼€å‘æ¡†æ¶ âœ¨ æ”¯æŒ QQã€QQé¢‘é“ã€Telegramã€ä¼å¾®ã€é£ä¹¦ã€é’‰é’‰ | çŸ¥è¯†åº“ã€MCP æœåŠ¡å™¨ã€OpenAIã€DeepSeekã€Geminiã€ç¡…åŸºæµåŠ¨ã€æœˆä¹‹æš—é¢ã€Ollamaã€OneAPIã€Dify\tagent,ai,chatbot,chatgpt,docker,gemini,gpt,llama,llm,mcp,openai,python,qq,qqbot,qqchannel,telegram\n",
    "A privacy-first, self-hosted, fully open source personal knowledge management software, written in typescript and golang.\tanki,chatgpt,deepseek,electron,evernote,knowledge-base,local-first,markdown,note-taking,notes-app,notion,obsidian,ocr,ollama,openai,pdf,s3,self-hosted,webdav\n",
    "Get your documents ready for gen AI\tai,convert,document-parser,document-parsing,documents,docx,html,markdown,pdf,pdf-converter,pdf-to-json,pdf-to-text,pptx,tables,xlsx\n",
    "The all-in-one Desktop & Docker AI application with built-in RAG, AI agents, No-code agent builder, MCP compatibility,  and more.\tai-agents,custom-ai-agents,deepseek,kimi,llama3,llm,lmstudio,local-llm,localai,mcp,mcp-servers,moonshot,multimodal,no-code,ollama,qwen3,rag,vector-database,web-scraping\n",
    "Streamlit â€” A faster way to build and share data apps.\tdata-analysis,data-science,data-visualization,deep-learning,developer-tools,machine-learning,python,streamlit\n",
    "Build and share delightful machine learning apps, all in Python. ğŸŒŸ Star to support our work!\tdata-analysis,data-science,data-visualization,deep-learning,deploy,gradio,gradio-interface,hacktoberfest,interface,machine-learning,models,python,python-notebook,ui,ui-components\n",
    "cuDF - GPU DataFrame Library\tarrow,cpp,cuda,cudf,dask,data-analysis,data-science,dataframe,gpu,pandas,pydata,python,rapids\n",
    "A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit floating point (FP8) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.\tcuda,deep-learning,fp8,gpu,jax,machine-learning,python,pytorch\n",
    "FlashInfer: Kernel Library for LLM Serving\tattention,cuda,distributed-inference,gpu,jit,large-large-models,llm-inference,moe,nvidia,pytorch\n",
    "CUDA Templates for Linear Algebra Subroutines\tcpp,cuda,deep-learning,deep-learning-library,gpu,nvidia\n",
    "MLX: An array framework for Apple silicon\tmlx\n",
    "DeepEP: an efficient expert-parallel communication library\t\n",
    "Fast and memory-efficient exact attention\t\n",
    "Development repository for the Triton language and compiler\t\n",
    "The Modular Platform (includes MAX & Mojo)\tai,language,machine-learning,max,modular,mojo,programming-language\n",
    "An open-source AI agent that brings the power of Gemini directly into your terminal.\tgemini,gemini-api\n",
    "AI coding agent, built for the terminal.\t\n",
    "Autonomous coding agent right in your IDE, capable of creating/editing files, executing commands, using the browser, and more with your permission every step of the way.\t\n",
    "an open source, extensible AI agent that goes beyond code suggestions - install, execute, edit, and test with any LLM\t\n",
    "â© Create, share, and use custom AI code assistants with our open-source IDE extensions and hub of rules, tools, and models\tai,chatgpt,copilot,developer-tools,intellij,jetbrains,llm,open-source,openai,pycharm,software-development,visual-studio-code,vscode\n",
    "ğŸ™Œ OpenHands: Code Less, Make More\tagent,artificial-intelligence,chatgpt,claude-ai,cli,developer-tools,gpt,llm,openai\n",
    "A reactive notebook for Python â€” run reproducible experiments, query with SQL, execute as a script, deploy as an app, and version with git. All in a modern, AI-native editor.\tartificial-intelligence,dag,data-science,data-visualization,dataflow,developer-tools,machine-learning,notebooks,pipeline,python,reactive,sql,web-app\n",
    "Lightweight coding agent that runs in your terminal\t\n",
    "Use your Neovim like using Cursor AI IDE!\t\n",
    "Production-ready platform for agentic workflow development.\tagent,agentic-ai,agentic-framework,agentic-workflow,ai,automation,gemini,genai,gpt,gpt-4,llm,low-code,mcp,nextjs,no-code,openai,orchestration,python,rag,workflow\n",
    "Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.\tai,apis,automation,cli,data-flow,development,integration-framework,integrations,ipaas,low-code,low-code-platform,mcp,mcp-client,mcp-server,n8n,no-code,self-hosted,typescript,workflow,workflow-automation\n",
    "The Postgres development platform. Supabase gives you a dedicated Postgres database to build your web, mobile, and AI applications.\tai,alternative,auth,database,deno,embeddings,example,firebase,nextjs,oauth2,pgvector,postgis,postgres,postgresql,postgrest,realtime,supabase,vectors,websockets\n",
    "RAGFlow is an open-source RAG (Retrieval-Augmented Generation) engine based on deep document understanding.\tagent,agentic,agentic-ai,agentic-workflow,ai,ai-search,deep-learning,deep-research,deepseek,deepseek-r1,document-parser,document-understanding,graphrag,llm,mcp,multi-agent,ollama,openai,rag,retrieval-augmented-generation\n",
    "Langflow is a powerful tool for building and deploying AI-powered agents and workflows.\tagents,chatgpt,generative-ai,large-language-models,multiagent,react-flow\n",
    "The TypeScript AI agent framework. âš¡ Assistants, RAG, observability. Supports any LLM: GPT-4, Claude, Gemini, Llama.\tagents,ai,chatbots,evals,javascript,llm,mcp,nextjs,nodejs,reactjs,tts,typescript,workflows\n",
    "AI Agents & MCPs & AI Workflow Automation â€¢ (280+ MCP servers for AI agents) â€¢ AI Automation / AI Agent with MCPs â€¢ AI Workflows & AI Agents â€¢ MCPs for AI Agents\tai-agent,ai-agent-tools,ai-agents,ai-agents-framework,mcp,mcp-server,mcp-tools,mcps,n8n-alternative,no-code-automation,workflow,workflow-automation,workflows\n",
    "ğŸ”¥ MaxKB is an open-source platform for building enterprise-grade agents.  MaxKB æ˜¯å¼ºå¤§æ˜“ç”¨çš„å¼€æºä¼ä¸šçº§æ™ºèƒ½ä½“å¹³å°ã€‚\tagent,agentic-ai,chatbot,deepseek-r1,knowledgebase,langchain,llama3,llm,maxkb,mcp-server,ollama,pgvector,qwen3,rag\n",
    "FastGPT is a knowledge-based platform built on the LLMs, offers a comprehensive suite of out-of-the-box capabilities such as data processing, RAG retrieval, and visual AI workflow orchestration, letting you easily develop and deploy complex question-answering systems without the need for extensive setup or configuration.\tagent,claude,deepseek,llm,mcp,nextjs,openai,qwen,rag,workflow\n",
    "Build AI Agents, Visually\tagentic-ai,agentic-workflow,agents,artificial-intelligence,chatbot,chatgpt,javascript,langchain,large-language-models,low-code,multiagent-systems,no-code,openai,rag,react,typescript,workflow-automation\n",
    "Python SDK, Proxy Server (LLM Gateway) to call 100+ LLM APIs in OpenAI format - [Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, Groq]\tai-gateway,anthropic,azure-openai,bedrock,gateway,langchain,litellm,llm,llm-gateway,llmops,mcp-gateway,openai,openai-proxy,vertex-ai\n",
    "The AI Toolkit for TypeScript. From the creators of Next.js, the AI SDK is a free open-source library for building AI-powered applications and agents\tanthropic,artificial-intelligence,gemini,generative-ai,generative-ui,javascript,language-model,llm,nextjs,openai,react,svelte,typescript,vercel,vue\n",
    "The most powerful and modular diffusion model GUI, api and backend with a graph/nodes interface.\tai,python,pytorch,stable-diffusion\n",
    "An open-source, code-first Python toolkit for building, evaluating, and deploying sophisticated AI agents with flexibility and control.\tagent,agentic,agentic-ai,agents,agents-sdk,ai,ai-agents,aiagentframework,genai,genai-chatbot,llm,llms,multi-agent,multi-agent-systems,multi-agents,multi-agents-collaboration\n",
    "ğŸŒ Make websites accessible for AI agents. Automate tasks online with ease.\tai-agents,ai-tools,browser-automation,browser-use,llm,playwright,python\n",
    "Model Context Protocol Servers\t\n",
    "Universal memory layer for AI Agents; Announcing OpenMemory MCP - local and secure memory management.\tagent,ai,aiagent,application,chatbots,chatgpt,embeddings,llm,long-term-memory,memory,memory-management,python,rag,state-management,vector-database\n",
    "Build resilient language agents as graphs.\t\n",
    "Agent Framework / shim to use Pydantic with LLMs\tagent-framework,llms,pydantic,python\n",
    "ğŸ¦œğŸ”— Build context-aware reasoning applications\tai,anthropic,gemini,langchain,llm,openai,python\n",
    "A powerful framework for building realtime voice AI agents ğŸ¤–ğŸ™ï¸ğŸ“¹\tagents,ai,openai,real-time,video,voice\n",
    "An Application Framework for AI Engineering\tartificial-intelligence,java,spring-ai\n",
    "LlamaIndex is the leading framework for building LLM-powered agents over your data.\tagents,application,data,fine-tuning,framework,llamaindex,llm,multi-agents,rag,vector-database\n",
    "Integrate cutting-edge LLM technology quickly and easily into your apps\tai,artificial-intelligence,llm,openai,sdk\n",
    "Open Source framework for voice and multimodal conversational AI\tai,chatbot-framework,chatbots,real-time,voice,voice-assistant\n",
    "A programming framework for agentic AI ğŸ¤– PyPi: autogen-agentchat Discord: https://aka.ms/autogen-discord Office Hour: https://aka.ms/autogen-officehour\tagentic,agentic-agi,agents,ai,autogen,autogen-ecosystem,chatgpt,framework,llm-agent,llm-framework\"\"\"\n",
    "\n",
    "# å®šä¹‰å¸¸ç”¨è¯åˆ—è¡¨\n",
    "common_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', \n",
    "                'of', 'with', 'by', 'from', 'up', 'about', 'into', 'over', 'after',\n",
    "                'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "                'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "                'this', 'that', 'these', 'those',\n",
    "                'your', 'my', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨å•è¯é¢‘ç‡\n",
    "word_freq = {}\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²\n",
    "words = []\n",
    "for word in text.lower().split():\n",
    "    # å¤„ç†ç ´æŠ˜å·åˆ†éš”çš„å•è¯\n",
    "    if '-' in word:\n",
    "        words.extend(word.split('-'))\n",
    "    else:\n",
    "        words.append(word)\n",
    "\n",
    "# ç¬¬äºŒæ­¥ï¼šæŒ‰é€—å·åˆ†å‰²\n",
    "comma_words = []\n",
    "for word in words:\n",
    "    if ',' in word:\n",
    "        comma_words.extend(word.split(','))\n",
    "    else:\n",
    "        comma_words.append(word)\n",
    "\n",
    "# æ›´æ–°wordsåˆ—è¡¨\n",
    "words = comma_words\n",
    "\n",
    "for word in words:\n",
    "    # å»é™¤æ ‡ç‚¹ç¬¦å·\n",
    "    word = word.strip('.,!?()[]{}\":;-/&')\n",
    "    # ç¡®ä¿å•è¯ä¸æ˜¯ç©ºå­—ç¬¦ä¸²ä¸”ä¸æ˜¯å¸¸ç”¨è¯\n",
    "    if word and word not in common_words:\n",
    "        # å°†å¤æ•°å½¢å¼è½¬ä¸ºå•æ•°å½¢å¼(ç®€å•çš„sç»“å°¾æƒ…å†µ)\n",
    "        if word.endswith('s') and word[:-1] in word_freq:\n",
    "            base_word = word[:-1]\n",
    "            word_freq[base_word] = word_freq.get(base_word, 0) + 1\n",
    "        else:\n",
    "            word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0555d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# æŒ‰é¢‘ç‡é™åºæ’åº\n",
    "sorted_word_freq = dict(sorted(word_freq.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "print(\"å»é™¤å¸¸ç”¨è¯åçš„è¯é¢‘ç»Ÿè®¡ç»“æœï¼ˆå‰100ä¸ªï¼‰ï¼š\")\n",
    "for word, freq in list(sorted_word_freq.items())[:100]:\n",
    "    print(f\"{word}: {freq}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
